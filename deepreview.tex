\documentclass[10pts]{article}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
%\usepackage[numbers]{natbib}
\usepackage{hyperref}

\addtolength{\textwidth}{2.5cm}
%\addtolength{\textwidth}{1cm}
\addtolength{\textheight}{2.5cm}
%\addtolength{\textheight}{1cm}
\addtolength{\topmargin}{-1.25cm}
\addtolength{\oddsidemargin}{-1.25cm}
\addtolength{\evensidemargin}{-1.25cm}

\newcommand{\deri}[2]{\frac{\partial  #1}{\partial #2}}
\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\citet}[1]{\cite{#1}}
\parskip 0.085in
\parindent 0.0in
\setcounter{secnumdepth}{1}
\title{Deep Learning}

\author{Yann LeCun, Yoshua Bengio and Geoffrey Hinton}
\date{}
\begin{document}

\maketitle
\vspace*{-6mm}

\section{Preface}

Deep Learning allows computational models that are composed of multiple processing 
layers to learn representions of data with multiple levels of abstraction.
These methods have dramatically improved the state-of-the-art
in speech recognition, visual object recognition, object detection,
and many other domains such as drug discovery and genomics. Deep
learning discovers intricate structure in large datasets by using the
{\em backpropagation algorithm} to indicate how a machine should
change its internal parameters that are used to compute the representation in each layer from
the representation in the previous layer.  Deep {\em convolutional nets}
have brought about dramatic improvements in processing
images, video, speech and audio, while {\em recurrent
  nets} have shone on sequential data such as text and
speech.
%Convolutional nets have quickly become the leading technology for image
%recognition and speech recognition, while recurrent nets are
%competitive in several natural language processing tasks, including
%language translation. 
%Combinations of recurrent and convolutional nets
%can generate good textual captions of images.

\section{Introduction}

% what is machine learning
Machine learning technology (ML) powers many aspects of modern
society: from web search to content filtering on social networks 
to recommendations on e-commerce websites, and it is increasingly
present in consumer products such as cameras and smartphones. ML
systems are used to identify objects in images, transcribe speech into
text, match news items, posts, or products with users' interests, and
select relevant results of search. Increasingly, these applications
make use of a class of techniques called {\em deep learning}.

% handcrafted features
Traditional ML techniques were limited in their ability to process
natural data in raw form. For decades, constructing a pattern
recognition or ML system required careful engineering and considerable
domain
expertise to design a {\em feature extractor} that transformed the raw
data, such as the pixel values of an image, into a suitable {\em
  internal representation}, or feature vector, from which the learning
subsystem, often a classifier, could detect or classify
patterns in the input.

% deep learning: learning representations
Representation learning is a set of methods that allows a machine to be fed with
raw data and to automatically discover the representations needed for
detection or classification. Deep learning methods are representation
learning methods with multiple levels of representation, obtained
by composing simple but non-linear modules that each transform the representation at one level
(starting with the raw input) into a representation at a higher, slightly more 
abstract level. With the composition of enough such transformations, very complex functions
can be learned. For classification tasks, the output representation
amplifies aspects of the input that are important for discrimination
and suppresses irrelevant variations. An image, for example, comes in the form of an array of
pixel values and the learned features in the first layer of representation
typically represent the presence
or absence of edges at  particular orientations and locations in
the image. The second layer typically detects motifs by spotting particular
arrangements of edges, regardless of small variations in the edge positions
The third layer  may assemble motifs into larger
combinations that correspond to parts of familiar
objects, and subsequent layers would detect objects as combinations of
these parts. The key aspect of deep learning is that these layers of
features are not designed by human engineers: They are learned from
data using a general purpose learning procedure.

% preview of deep learning results
Deep learning is making major advances in solving problems that have
resisted the best attempts of the Artificial Intelligence community
for many years. It has turned out to be very good at discovering
intricate structures in high-dimensional data and is therefore
applicable to many domains of science, business, and government. In
addition to beating records in image
recognition~\citep{Krizhevsky-2012-small,farabet-pami-13,tompson-nips-14,szegedy-2014}
and speech
recognition~\citep{Mikolov-ASRU-2011,Hinton-et-al-2012,Sainath-et-al-ICASSP2013}, it has
beaten other ML techniques at predicting the activity of potential
drug molecules~\citep{Ma-et-al-2015}, detecting new fundamental
particles~\citep{Ciodaro-et-al-2012,Melis-Higgs-boson-competition-2014},
% YB thinks that Ciodaro-et-al might be {YANNparticles} 
% classifying galaxies, ~\citep{YANNgalaxies}, % YB while waiting for bib entry, 
reconstructing brain circuits~\citep{helmstaedter-nature-2013}, and
predicting the effects of mutations in non-coding DNA on gene
expression and disease~\citep{Keung-et-al-2014,Xiong-et-al-Frey-science2015}.
Perhaps more surprisingly, deep learning has produced extremely
promising results for a variety of tasks in natural language
understanding~\citep{collobert:2011b}, particularly topic
classification, %~\citep{topic-classification},  % YB while waiting for bib entry
sentiment
analysis,% cutting citations ~\citep{Glorot+al-ICML-2011-small}, 
question-answering~\cite{Bordes-et-al-EMNLP2014}, and language
translation~\cite{Jean-et-al-ACL2015,Sutskever-et-al-NIPS2014}.

% our conclusions (which Nature insists on having at the end of the intro).
We believe that deep learning will have many more successes in the
near future because it requires very little hand-engineering so it can
easily take advantage of increases in the amount of available
computation and data. New learning algorithms and new architectures
that are currently being developed for deep neural networks will only
accelerate this progress.

\section{Supervised Learning}

% supervised learning, objective function, gradient.
The most common form of machine learning, deep or not, is {\em
  supervised learning}. Imagine that we want to build a system that
can classify images as containing, say, a house, a car, a person, or a
pet. We first collect a large dataset of images of houses, cars,
persons and pets, each labeled with its category. During training, the
machine is shown an image and produces an output in the form of
a vector of scores, one for each category. We want the desired
category to have the highest score of all categories, but this is
unlikely to happen before training.  We compute an {\em objective
  function}, that measures the error (or distance) between the output
scores and the desired pattern of scores. The machine then modifies
its internal adjustable parameters so as to reduce this error. These
adjustable parameters, often called weights, are real numbers which
can be seen as ``knobs'' that define the input-output function of the
machine. In a typical deep learning system, there may be hundreds of
millions of these adjustable weights, and hundreds of millions
of labeled examples with which to train the machine.

To properly adjust the weight vector, the learning algorithm computes
a {\em gradient vector} which, for each weight, indicates by what
amount the error would increase or decrease if the weight were
increased by a tiny amount. The weight vector is then adjusted in the
opposite direction to the gradient vector. 

% GD
The objective function, averaged over all the training examples, can
be seen as a kind of hilly landscape in the high-dimensional space of
weight values. The negative gradient vector indicates the direction of
steepest descent in this landscape, taking it closer to a minimum
where the output error is low on average.

% SGD
In practice, most practitioners use a procedure called {\em Stochastic
  Gradient Descent} or SGD. It consists in showing the input vector
for a few examples, computing the outputs and the errors, computing
the average gradient for those examples, and adjusting the
weights. The process is repeated for many small sets of examples
from the training set until the average of the objective function stops
decreasing. It is called stochastic because each small set of examples
gives a noisy estimate of the average gradient over all examples. In
practice, this simple procedure usually finds a good set of weights
surprisingly quickly when compared with far more elaborate
optimization techniques~\cite{bottou-bousquet-2008-small}. After
training, the performance of the system is measured on a different set
of examples called a {\em test set}. This serves to test the {\em
  generalization ability} of the machine, {\it i.e.} its ability to
produce sensible answers on new inputs that it has never seen during
training.

% shallow linear classifiers
Many of the current practical applications of ML use linear classifiers on top of
hand-engineered features. A two-class linear classifier computes a
weighted sum of the feature vector components. If the weighted sum is
above a threshold, the input is classified as belonging to a
particular category. % Multiple classes can be handled by using multiple
% two-class classifiers, each classifying one class against all
% others. 
% The learning procedure adjusts the weights of the linear
% combination, for which the gradient is very easy to compute. Linear
% classifiers have been widely used for many decades, starting with the
% Perceptron model~\citep{Rosenblatt57} and the still-popular logistic
% regression~\cite{Hastie2001}.
% , and the linear support vector
% machine~\cite{Vapnik+Chervonenkis-1964}. %{Boser92}. 

% features: solving the selectivity-invariance dilemma
It has been known since the 1960s that linear classifiers can only
carve their input space into very simple regions, namely
half-spaces separated by a hyperplane~\citep{Duda-Hart}.  But problems
such as image and speech recognition require the input-output function
to be insensitive to irrelevant variations of the input, such as
variations in position, orientation or illumination of an object, or
variations in the pitch or accent of speech, while being very sensitive to
particular minute variations, such as the difference between a white
wolf and a samoyed (a breed of wolf-like white dog).  At the pixel
level, images of two samoyeds in different poses and in different
environments may be very different from each other, while two images
of a samoyed and a wolf in the same position and on similar
backgrounds may be very similar to each other. A linear classifier,
or any other ``shallow'' classifier operating on raw pixels could not
possibly distinguish the latter two, while putting the former two in
the same category. This is why shallow classifiers require a good
feature extractor that solves the {\em selectivity-invariance dilemma},
{\it i.e.} that produces representations that are {\em selective} to the
aspects of the image that are important for discrimination while being {\em invariant} to 
irrelevant aspects like the pose of the animal.
%
% classical extensions of simple classifiers
% YB: I propose to mostly remove the paragraph below (but keep the Scholkopf book ref)
% There have been numerous attempts, going back to the early 1960s, to
% increase the flexibility of classifiers so as to allow them to
% separate categories from one another by more complex surfaces than
% flat hyperplanes. A popular example is the class of kernel methods~\citep{Scholkopf+Smola-2002book}, % added ref for BS
%particularly the Support Vector Machine~\citep{Boser92} which
% represents inputs by a vector whose components are similarities to
% each of the training samples computed through a so-called {\em kernel
%   function}. While these methods are good general-purpose classifiers,
% they do not perform well for tasks such as image or speech recognition
% in which the input space is very high-dimensional and the input-output
% mapping is so complex that computing similarities using a simple
% kernel function is ineffective. 
To make classifiers more powerful, one can use generic non-linear features,
as with kernel methods~\citep{Scholkopf+Smola-2002book}, but generic
features such as those arising with the Gaussian kernel do not allow the learner to
generalize well far from the training examples~\citep{Bengio-localfailure-NIPS-2006-small}.
The traditional option is to design good feature extractors, which requires a considerable amount
of engineering skill and domain expertise. But this can all be avoided if
good features can be learned automatically using a general purpose
learning procedure.  That is the key advantage of deep
learning.

% deep architectures

A deep learning architecture is a multilayer stack of modules, all (or
most) of which are subject to learning, and many of which compute
non-linear input-output mappings.  Each module in the stack transforms
its input so as to increase both the selectivity and the invariance of
the representation. With multiple non-linear layers, say 5 to 20, a system
can implement extremely intricate functions of its inputs that are
simultaneously sensitive to minute details-- distinguishing samoyeds
from wolves-- and insensitive to large irrelevant variations such as the
background, pose, lighting, and surrounding objects.

% backprop
\section{Backpropagation: a simple way to train multilayer architectures} 

From the earliest days of pattern
recognition~\citep{selfridge,Rosenblatt57} researchers have aimed to
replace hand-engineered features by trainable multilayer networks,
but despite its simplicity, the solution was not widely
understood until
the mid 1980s.  
% and was largely forsaken by the ML community until recently.
As it turns out, multilayer architectures can be
trained by simple stochastic gradient descent. As long as the modules are
relatively smooth functions of their inputs and of their internal
weights, one can compute gradients using the {\em backpropagation
  procedure}.
The idea that this could be done, and that it worked, was discovered independently by
several different groups in the 1970s and
1980s~\citep{Werbos74,Parker85,LeCun85,RHW}. 
% It spurred a wave of interest for
% neural networks, becoming quite popular in the late 1980s, but falling out of favor
% between the mid 1990s until their renaissance in the late 2000s.

The backpropagation procedure to compute the gradient of an objective
function with respect to the weights of a multilayer stack of modules
is nothing more than a practical application of the chain rule for
derivatives. The key insight is that the derivative (or gradient) of
the objective with respect to the {\it input} of a module can be
computed by working backwards from the gradient with respect to the
{\em output} of that module (or the input of the subsequent
module), as illustrated in Figure~\ref{fig:backprop-box}.
% Basically, take a module $M$ inside the network, whose input
%is $x$, weight vector is $w$, and output is $y = M(x,w)$. If one knows
%the gradient $\deri{E}{y}$ of the objective function $E$ with respect
%to the module's output $y$, one can blindly apply chain rule to
%compute the gradient with respect to the input:
%\[
%  \deri{E}{x} = \deri{y}{x}\deri{E}{y}
%\]
%This equation works even when $x$ and $y$ are vectors, in which case
%$\deri{E}{x}$ and $\deri{E}{y}$ are gradient vectors, and
%$\deri{y}{x}$ is a {\em Jacobian matrix} of $M$ where element $(i,j)$
%indicates by how much output $j$ would increase proportionally if
%input $i$ were increased by a tiny amount $\Delta y_j =
%\left(\deri{y}{x}\right)_{ij} \Delta x_i$. 
The backpropagation equation can be applied repeatedly to propagate
gradients through all modules, starting from the output at the top 
(where the network produces its prediction) all the way to the bottom
(where the external input is fed). Once these
gradients have been computed, it is straightforward to compute the gradients 
with respect to the weights of each module. 
%Figure~\ref{fig:backprop-box})
%shows how this works out when the network is composed of layers of
%weight matrices interspersed with scalar non-linearities.
%% the notation in the figure should be updated.

% deep neural nets
Many applications of deep learning use feedforward neural network
architectures, illustrated in Figure~\ref{fig:backprop-box}, which
learn to map a fixed-size input ({\it e.g.} an image) to a fixed-size
output ({\it e.g.} a probability for each of several categories). To
go from one layer to the next, a set of units compute a weighted sum
of their inputs from the previous layer and pass the result through a
non-linear function. Currently, the most popular non-linear function
is the rectified linear unit ({\em ReLU}) which is simply
the half-wave rectifier $f(z) = \max(z,0)$. In past decades, neural
nets used smoother non-linearities, such as $\tanh(z)$ or
$1/(1+\exp(-z))$ but the ReLU typically learns much faster in networks
with many layers, allowing to train a deep supervised network
without unsupervised pre-training~\citep{Glorot+al-AI-2011-small}. 
Units that are not in the input or output layer are
traditionally called {\em hidden units}. The hidden layers can be seen
as distorting the input in a non-linear way so that categories become
linearly separable by the last layer, as indicated in
figure~\ref{fig:backprop-box}.



%\begin{figure}[htp]
\begin{figure}[b]
\fbox{
\begin{minipage}{\textwidth}
\centerline{
\fbox{\includegraphics[width=0.69\linewidth]{netvis-simple-2S.png}}
\fbox{\includegraphics[width=0.28\linewidth]{chain-rule.pdf}}}
\centerline{
\hspace*{.5mm}\fbox{\includegraphics[width=0.49\linewidth]{bpfig_forward.pdf}}
\hspace*{0mm}\fbox{\includegraphics[width=0.49\linewidth]{bpfig_backward.pdf}}
}
\caption{
{\bf Top left.} A multi-layer neural network can distort the
input space to make the classes (whose examples are supposed to
be on the red and blue lines, here) linearly separable. Note how
a regular grid in input space is also transformed. This is an
illustrative example with only two input units, two hidden units
and one output unit, but the networks used for object recognition
or natural language processing contain tens or hundreds of thousand
of units. Reproduced with permission by Chris Olah from~\url{http://colah.github.io/}.
\newline
{\bf Top right.} The chain rule of derivatives tells us how
two small effects (that of a small change of $x$ on $y$, and that of $y$ on $z$)
are composed. A small change $\Delta x$ in $x$
gets transformed first into a small change $\Delta y$ in $y$
by getting multiplied by $\frac{\partial y}{\partial x}$
(that is the definition of partial derivative). Similarly,
the change $\Delta y$ creates a change $\Delta z$ in $z$. Substituting one equation
into the other gives the chain rule of derivatives, i.e., how
$\Delta x$ gets turned into $\Delta z$ through multiplication
by the product of $\frac{\partial y}{\partial x}$ and
$\frac{\partial z}{\partial x}$. It also works when $x$, $y$
and $z$ are vectors (and the derivatives are Jacobian matrices). 
\newline
{\bf Bottom left.} 
The equations used for computing the forward pass in a neural
net with two hidden layers and one output layer, each constituting
a module through which one can backpropagate gradients. At each layer, we first compute the total
input $z$ to each unit, which is a weighted sum of the outputs of the
units in the layer below. Then a non-linear function $f(.)$ is applied
to $z$ to get the output of the unit.  For simplicity, we have omitted
bias terms. The non-linear functions used in neural networks include 
the rectified linear unit (ReLU) $f(z) = max(0, z)$, commonly-used in 
recent years, as well as the more traditional sigmoids, such as the 
hyberbolic tangent, $f(z) = (\exp(z)-\exp(-z))/(\exp(z)+\exp(-z))$ 
and logistic function logistic, $f(z) = 1/(1+\exp(-z))$.
{\bf Bottom right.} 
The equations used for computing the backward pass. At each
hidden layer we compute the error-derivative w.r.t. the output of each
unit which is a weighted sum of the error-derivatives w.r.t. the total
inputs to the units in the layer above. We then convert the
error-derivative w.r.t. the output into the error-derivative w.r.t.
the input by multipling it by the gradient of $f(z)$.  At the output
layer, the error-derivative w.r.t. the output of a unit is computed by
differentiating the cost function. This gives $y_l-t_l$ if the cost
function for unit $l$ is $0.5(y_l-t_l)^2$ where $t_l$ is the target
value. Once the $\partial E/\partial z_k$ is known, the
error-derivative for the weight $w_{jk}$ on the connection from unit
$j$ in the layer below is just $y_j \partial E/\partial z_k$.
}
\label{fig:backprop-box}
\end{minipage}
}
\end{figure}

% the reduced history section

In the late 1990s, neural nets and backpropagation were largely
forsaken by the machine learning community and ignored by the computer
vision and speech recognition communities.  It was widely believed
that learning useful, multi-stage, feature extractors with little prior knowledge 
was infeasible. In particular, it was commonly
believed that simple gradient descent would get trapped in poor
local minima -- weight configurations for which no small change would
reduce the average error.
%We now know
%that the main reasons for the limited success of backpropagation in
%deep networks with dense connectivity were that the labeled datasets were typically much too
%small and the computers were much too slow. Additional reasons were
%that researchers were not initializing the weights properly and were
%not using the type of non-linearlity that learns most
%rapidly~\citep{lecun-98b,GlorotAISTATS2010-small}.

In practice, poor local minima are rarely a problem with large networks. Regardless
of the initial conditions, the system nearly always reaches solutions
of very similar quality.  Recent results strongly suggest the
landscape is packed with a combinatorially large number of {\em saddle
  points} where the gradient is zero, and the surface curves up in
most dimensions and curves down in the 
remainder~\citep{Dauphin-et-al-NIPS2014-small,Choromanska-et-al-AISTATS2015}.
The analysis seems to show that saddle points with
only a few downward curving directions are present in very large
numbers, but almost all of them have very similar values of the
objective function.  Hence it doesn't much matter which of these
saddle points the algorithm gets stuck at.
%% Avoiding being trapped near saddle points seems more important.

%% It also took an inordinate amount of time for the community to adopt
%% stochastic gradient methods for large-scale machine learning. Key
%% theoretical and practical work showed that, while SGD has terrible
%% asymptotic convergence properties, it is extremely efficient for the
%% kind of approximate optimization required in ML
%% applications~\citep{bottou-bousquet-2008-small}. But when training a
%% large ConvNet can take days or weeks on the fastest GPUs, one is eager
%% to discover more efficient optimization methods that use the curvature
%% information the past update directions~\citep{lecun-98b,Montavon2012}.

Interest in deep feedforward networks was revived around
2006~\citep{IJCAI,Hinton06-small,Bengio-nips-2006-small,ranzato-07-small}
by a group of researchers brought together by the Canadian Institute
for Advanced Research (CIFAR) who introduced
unsupervised learning procedures that could
create layers of feature detectors without requiring labeled data. The
objective in learning each layer of feature detectors was to be able
to reconstruct or model the activities of feature detectors (or raw
inputs) in the layer below.  By ``pre-training'' several layers of
progressively more complex feature detectors using this reconstruction
objective, the weights of a deep network could be initialized to
sensible values.  A final layer of output units could then be added to
the top of the network and the whole deep system could be fine-tuned
using standard backpropagation
\citep{Hinton-Science2006,Bengio-nips-2006-small,ranzato-07-small}.
This worked remarkably well for recognizing handwritten digits or for
detecting pedestrians, especially when the amount of labeled data was
very limited~\citep{sermanet-cvpr-13}.

% The parallelization of large neural nets over a single GPU has
% required to use ``mini-batches'' of samples over which the gradients
% are averaged. The purpose of this is to saturate the GPU. 
% But one big
% unsolved question is the best way to scale up the training of very large neural
% nets over multiple GPU cards across multiple machines. As of today, this
% question is still unresolved.

The first major application of this pre-training approach was in
speech recognition and it was made possible by the advent of fast
Graphics Processing Units (GPUs) that were convenient to
program~\citep{RainaICML09-small} and allowed researchers to train
networks 10 or 20 times faster.  In 2009, the approach was used to map
short temporal windows of coefficients extracted from a soundwave to a
set of probabilities for the various fragments of speech that might be
represented by the frame in the center of the window.  It achieved
record-breaking results on a standard speech recognition benchmark
that used a small vocabulary~\citep{TIMITpaper} and was quickly
developed to give record-breaking results on a large vocabulary
task~\citep{Dahl2012}.  By 2012, versions of the deep net from 2009
were being developed by many of the major speech
groups~\citep{Hinton-et-al-2012} and were already being deployed in
Android phones.  For smaller datasets, unsupervised pre-training helps
to prevent overfitting~\citep{Bengio-Courville-Vincent-TPAMI2013},
leading to significantly better generalization when the number of
labeled examples is small, or in a transfer setting where we have lots
of examples for some ``source'' tasks but very few for some ``target''
task.  Once deep learning had been rehabilitated, it turned out the
pre-training stage was only needed for small datasets.

There was, however, one particular type of deep, feed-forward network
that was much easier to train and generalized much better than
networks with full connectivity between adjacent layers. This was the
convolutional neural network~\cite{lecun-90c,lecun-98}. It achieved
many practical successes during the period when neural networks were
out of favor and it has recently been widely adopted by the computer
vision community. 

\section{Convolutional neural networks}

Convolutional neural networks (ConvNets) are designed to process
data that come in the form of multiple arrays, for example a color
image composed of three 2D arrays containing pixel intensities in the
three color channels. Many data modalities are in the form of multiple
arrays: 1D for signals and sequences including language, 2D for images
or audio spectrograms, 3D for video or volumetric images.  There are
four key ideas behind ConvNets that take advantage of the properties
of natural signals: local connections, shared weights,
pooling, and the use of many layers.

The architecture of a typical ConvNet is shown in
figure~\ref{fig:convnet}.  It is structured as a series of stages. The
first few stages are composed of two types of layers: convolutional
layers and pooling layers. Units in a convolutional layer are
organized in {\em feature maps}, within which each unit is connected
to local patches in the feature maps of the previous layer through a
set of weights called a {\em filter bank}. The result of this local
weighted sum is then passed through a non-linearity such as a ReLU.  All
units in a feature map share the same filter bank. Different feature
maps in a layer use different filter banks. The reason for this
architecture is twofold. First, in array data such as images, local
groups of values are often highly correlated forming distinctive local
motifs that are easily detected. Second, the local statistics of
images and other signals are invariant to location. In other word, if
a motif can appear in one part of the image, it could appear anywhere,
hence the idea of units at different locations sharing the same
weights, and detecting the same pattern in different parts of the
array.  Mathematically, the filtering operation performed by a feature
map is a {\em discrete convolution}, hence the name.

While the role of the convolutional layer is to detect local
conjunctions of features from the previous layer, the role of the
pooling layer is to merge semantically similar features into
one. Since the relative positions of the features forming a motif can
vary somewhat, reliably detecting the motif can be done by
coarse-graining the position of each feature. A typical pooling unit
computes the max of a local patch of units in one feature map (or in a
few feature maps). Neighboring pooling units take input from patches
that are shifted by more than one row or column, thereby reducing the
dimension of the representation and creating an {\em invariance} to
small shifts and distortions.  Two or three stages of convolution,
non-linearity, and pooling are stacked, followed by more convolutional
and fully-connected layers.  Back-propagating gradients through a
ConvNet is as simple as through a regular deep network, allowing all
the weights in all the filter banks to be trained. 
% The error
% derivative for a shared weight is just the sum the derivatives for all
% the instances of that weight.

Deep neural networks exploit the property that many natural
signals are {\em compositional hierarchies}, where higher-level
features are obtained by composing lower-level ones. In images, local
combinations of edges form motifs, motifs assemble into parts, and
parts form objects.  Similar hierarchies
exists in speech and text from sounds to phones, phonemes, syllables,
words, and sentences. The pooling allows representations to vary very little
when elements in the previous layer vary in position and appearance. 

%\begin{figure}[htp*]
\begin{figure}[b]
\vspace{-4mm}
\begin{center}
  \includegraphics[width=0.70\linewidth]{convnet-diagram-labeled}
\end{center}
\vspace{-3mm}
\caption{The {\em outputs} (not the filters) of each layer (each column of the figure) 
of a typical convolutional network architecture
applied to the image of a handwritten digit (left). Each rectangular
image is a feature map, corresponding to the output for one of the learned features 
detected at each of the image positions.
Each column corresponds to a layer, with information flowing from the left to
the right. We see that lower level features mostly act as oriented edge detectors.
The last layer is fully connected so it has no spatial structure.
}
\vspace{-2mm}
\label{fig:convnet}
\end{figure}

The convolutional and pooling layers in ConvNets are directly inspired
by the classic notions of simple cells and complex cells in visual
neuroscience~\citep{Hubel62}, and the overall architecture is
reminiscent of the LGN-V1-V2-V4-IT hierarchy in the visual cortex 
ventral pathway~\citep{Felleman+VanEssen-1991}. When ConvNet models and
monkeys are shown the same picture, the activations of high-level
units in the ConvNet explains half the variance of random sets of 160
neurons in the monkey's
inferro-temporal cortex~\citep{cadieu-plos-2014}. ConvNets have their roots
in Fukushima's Neocognitron~\citep{fukushima-82} whose architecture
was somewhat similar but did not have an end-to-end supervised
learning algorithm such as backpropagation. 
A primitive one-dimensional convnet called a time-delay
neural net was used for the recognition of phonemes and simple 
words~\cite{Waibel89b,bottou-89}

There have been numerous applications of convolutional networks going
back to the early 1990s, starting with time-delay neural networks for
speech recognition~\citep{Waibel89b} and document reading~\citep{lecun-98}. 
The document reading system used a ConvNet trained jointly with
a probabilistic model that implemented language constraints. By the
late 1990s this system was reading over 10\% of all the checks in the
US. A number of ConvNet-based OCR and handwriting recognition systems
were later deployed by
Microsoft~\citep{simard-03-small}.
% including for Arabic and
% Chinese~\citep{abdulkader-iwfhr-06,chellapilla-iwfhr-06a}. 
ConvNets
were also experimented with in the early 1990s for object detection 
in natural images, including faces and
hands~\cite{vaillant-monrocq-lecun-94,nowlan-platt-95} and for face
recognition~\cite{lawrence-tnn-1997}.

\section{Image understanding with deep convolutional networks}

Since the early 2000s, ConvNets have been applied with great 
success to the detection, segmentation, and
recognition of objects and regions in images. These were all
tasks in which labeled data was relatively abundant, such as traffic
sign recognition~\cite{Ciresan-et-al-2012}, the
segmentation of biological images~\cite{ning-05} particularly for
connectomics~\cite{Turaga2010}, and the detection of faces, text,
pedestrians, and human bodies in natural
images~\cite{vaillant-monrocq-lecun-94,nowlan-platt-95,garcia-delakis-04,osadchy-07,sermanet-cvpr-13,tompson-cvpr-15}.
A major practical success of ConvNets in recent times is face
recognition~\citep{Taigman-et-al-CVPR2014}. 

An important application is the labeling of images at the pixel level,
the applications of which include autonomous mobile robots and
self-driving cars~\cite{hadsell-jfr-09,farabet-icml-12}. Companies
such as Mobileye and NVIDIA are using such ConvNets-based methods in
their upcoming vision systems for cars.  Other applications of
ConvNets that are gaining importance concern natural language
understanding~\citep{collobert:2011b}, and speech
recognition~\cite{Sainath-et-al-ICASSP2013}.

Despite these successes, ConvNets were largely forsaken by the
mainstream computer vision and machine learning communities until the
ImageNet competition in 2012.  When deep convolutional networks were
applied to a dataset of about a million images from the web that
contained a thousand different classes they achieved spectacular
results, almost halving the error rates of the best competing
approaches~\citep{Krizhevsky-2012-small}.  This success came from
efficient use of GPUs, ReLUs, a new regularization technique
called ``dropout'' ~\citep{Srivastava14}, and techniques to
generate more training examples by deforming the existing ones.
This success has brought about a
revolution in computer vision, and ConvNets are now the dominant
approach for almost all recognition and detection
tasks~\citep{sermanet-iclr-14,girshick-cvpr-2014,Taigman-et-al-CVPR2014,simonyan-arxiv-2014,szegedy-2014,tompson-cvpr-15}
and approach human performance on some tasks. A recent stunning
demonstration combines ConvNets and neural net modules for the
generation of image captions, as shown in
Figure~\ref{fig:caption-generation}.

Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds
of millions of weights, and billions of connections between
units. While training such large networks could take weeks only two
years ago, progress in hardware, software, and algorithm
parallelization have reduced training times to a few hours.

The performance of ConvNet-based vision systems has caused most major
technology companies to initiate research and development projects and
to deploy ConvNet-based image understanding products and services
including Google, Facebook, Microsoft, IBM, Yahoo!, Twitter, Adobe,
and a quickly-growing number of startups.


%% If ConvNets and other neural nets existed in the 1990s, why did they become so popular and
%% dominant only now? It was initially thought that training of neural nets
%% would get stuck in poor solutions (called local minima) due to non-convexity
%% of the training objective, whereas the alternative learning framework
%% of {\em kernel machines}~\citep{Scholkopf+Smola-2002book} was more appealing
%% because it involved a convex optimization. As discussed below, it is only
%% in 2014 that theoretical and empirical evidence have suggested that local
%% minima are probably not as much of a problem as was initially 
%% thought~\citep{Dauphin-et-al-NIPS2014-small,Choromanska-et-al-AISTATS2015}.
%% Much before that realization, though, a number of 
%% ideas and technologies over the last decade enabled the success. 
%% First, supervised ConvNets work best
%% when training data is abundant, and large labeled image datasets such
%% as ImageNet~\citep{imagenet_cvpr09} only appeared recently. Second,
%% the appearance of GPU cards capable of over 1Tflops, together with
%% GPU-optimized software for ConvNet made it possible to train these
%% large networks in a few weeks (now days) of computer time. Third, the
%% use of rectifying non-linearities instead of sigmoids allowed to train
%% much deeper networks than previously possible without unsupervised
%% pre-training~\cite{jarrett-iccv-09,Glorot+al-AI-2011-small}. Lastly,
%% the ``drop out'' regularization method was shown
%% to mitigate the effects of over-fitting in very large networks. 
%% 
%% Deep convolutional nets have quickly replaced the carefully engineered
%% pipelines full of hand-designed features developed by the computer
%% vision community and they are now deployed in many computer vision
%% applications, including image tagging, object detection, face
%% recognition, facial expression recognition and scene parsing.
%% 

ConvNets are easily amenable to efficient hardware implementations in
chips or Field-Programmable Gate
Arrays~\cite{boser-92,farabet-suml-11}.  A number of companies such as
NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet
chips to enable real-time vision applications in smart phones,
cameras, robots, and self-driving cars.


%% YLC: I find that this section would be cleared if it followed
%% the section on NLP applications. 

\section{Distributed representations and language processing}

%representations allow a lot of parallel computation at the feature
%level so that complex tasks can be solved in a very small number of
%sequential steps.
Deep learning theory shows that deep nets have two different
exponential advantages over classical learning algorithms that do not
use distributed
representations~\citep{Bengio-localfailure-NIPS-2006-small}. Both of
these advantages arise from the power of {\em composition} and depend
on the underlying data generating distribution having an appropriate
componential structure~\citep{Bengio-Courville-Vincent-TPAMI2013}.
First, learning distributed representations enables generalization to
new combinations of the values learned features beyond those seen
during training (e.g. $2^n$ combinations
are possible with $n$ binary features) ~\citep{Bengio-2009-book,Montufar+Morton-2014}.
Second composing layers of representation in a deep net brings the potential for another
exponential advantage~\citep{Montufar-et-al-NIPS2014} (exponential in
the depth).

The hidden layers of a multi-layer neural network learn to represent
the network's inputs in a way that makes it easy to predict the target
outputs. This is nicely demonstrated by training a multi-layer neural
network to predict the next word in a sequence from a local context of
earlier words~\citep{BenDucVin01-short}.  Each context word is
presented to the network as a one-of-N vector, i.e., in which one
component has a value of $1$ and the rest are $0$. 
%The output layer
%has one unit per word and is forced to produce a probability
%distribution over words by using the ``softmax'' function:
%\begin{equation}
%p_j = \frac{e^{z_j}}{\sum_i e^{z_i}}
%\end{equation}
%where $z_j$ is the total input received by output unit $j$. 
%\iffalse
%The hidden
%layers use a non-linearity such as the logistic, tanh, or rectifier, respectively:
%\begin{equation}
%y_j = \frac{1}{1+ e^{z_j}}, \ \ \ y_j = \frac{e^{z_j}-e^{-z_j}}{e^{z_j}+e^{-z_j}}, \ \ \
% y_j = \max(0, z_j).
%\end{equation}
%\fi
%
%The presence of a particular word at the $i$-th position in the input
%of the network sets the value of specialized hidden units to values
%that correspond to the representation of that word. 
In the first layer, each word
creates a different pattern of activations, or word-vector, a distributed
representation illustrated
in Figure~\ref{fig:word-embeddings}.  In a language model, the other
layers of the network learn to convert the input word-vectors into an
output word-vector for the predicted next-word, which can be used to
predict the probability for any word in the vocabulary to appear as
the next word.  The network learns word-vectors that contain many
active components each of which can be interpreted as a separate
feature of the word, as was first demonstrated by~\citet{RHW} in the context
of learning distributed representations for symbols.
%. This was first demonstrated using sequences of
%three words, such as (Colin, Mother, Victoria), derived from two
%family trees~\citep{RHW}. The learned word vector for Colin contained
%``semantic'' features representing which family tree Colin was in,
%what branch of that tree he was in, and what generation he was from.
%% TWO NATIVE ENGLISH SPEAKERS AGREE THAT THE "FROM" SHOULD STAY AT THE
%% END IN ORDER NOT TO SOUND STILTED.
These semantic features were not explicitly present
in the input.  They were discovered by the learning procedure as a good way
of factorizing the structured relationships between the input and output
symbols into multiple ``micro-rules''.  
%For example, given Colin's
%generation and the learned feature of Mother that the output should be one
%generation higher than the input, we can already restrict the answer to
%people in a specific generation. The ability of deep learning to discover
%many such micro-rules allows it to generalize well to novel combinations of
%individually familiar context words.
%
%The family tree demonstration used a small, clean dataset in which the
%regularities could be captured by micro-rules that were never violated
%and the features were easy to interpret, 
Learning word-vectors turned out to also
work very well when the word sequences come from a large corpus of
real text and the individual micro-rules are
unreliable~\citep{BenDucVin01-short}. When trained to predict the next
word in a news story, for example, the learned word vectors for
Tuesday and Wednesday are very similar, as are the word vectors for
Sweden and Norway.
%% This is not to say that Tuesday and Wednesday are
%% put in the same category, but rather that they share many semantic and
%% grammatical attributes, so that what the neural network learns from
%% one day of the week can often be generalized to the other, while
%% individual days remain different (for example one imagines that the
%% neural network also learns some features that distinguishes week-end
%% days and weekdays, while Saturday and Sunday might have special roles
%% in some religions, etc.).  
Such representations are called {\em
  distributed representations} because their elements (the features) are
not mutually exclusive and their many configurations
%% exists a rich and vast set of
%% possible configurations of values of these features, 
correspond to
the variations seen in the observed data.  These word-vectors are
composed of
{\em learned features} that were not determined ahead of time by
experts, but automatically discovered by the neural network.  Vector
representations of words learned from text are now very widely used in
natural language
applications~\citep{Schwenk-2007,collobert:2011b,Socher-2011,
  Mikolov-et-al-NIPS2013,Bahdanau-et-al-ICLR2015-small,Sutskever-et-al-NIPS2014}.

The issue of representation lies at the heart of the debate between
the logic-inspired and the neurally-inspired paradigms for
cognition. In the logic-inspired paradigm, an instance of a symbol is
something whose only property is that it is either identical or
non-identical to other symbol instances. It has no internal structure
that is relevant to its use and  to reason with symbols, they must be
bound to the variables in judiciously chosen rules of inference.  By
contrast, neural networks just use big activity vectors, big weight
matrices and scalar non-linearities to perform the type of fast
``intuitive'' inference that underpins effortless commonsense
reasoning.


%% YLC: we need to talk about word2vec somewhere around here.
%% it's so popular that we can't just not mention it.

Before the introduction of neural language
models~\citep{BenDucVin01-short}, the standard approach to statistical
modeling of language did not exploit distributed representations: it 
was based on counting frequencies of occurrences of
short symbol sequences (called N-grams). 
The number of possible N-grams is $V^N$ where $V$ is
the vocabulary size, so taking
into account a context of more than a handful of words would require
very large training corpora. 
N-grams treat each word as an atomic unit, so they cannot generalize across
semantically related sequences of words, whereas
neural language models can generalize across semantically related
contexts because they associate each word with a vector of real
valued features
and semantically related
words end up close to each other in that vector space, as illustrated in
Figure~\ref{fig:word-embeddings}. 


%\begin{figure}[ht]
\begin{figure}[b]
\fbox{
\begin{minipage}{\textwidth}
\centerline{
\includegraphics[width=0.49\linewidth]{word-embeddings.png}
\includegraphics[width=0.49\linewidth]{phrase-embeddings.pdf}
}
\caption{Left: Illustration of word representations learned for modeling
language, non-linearly projected to 2-D for visualization using the
t-SNE algorithm~\citep{VanDerMaaten08}. Right: 2-D representation
of phrases learned by an English-to-French encoder-decoder recurrent
neural network or RNN~\citep{Bahdanau-et-al-ICLR2015-small}. One can observe that
semantically similar words or sequences of words are  mapped to
nearby representations. The distributed representations of
words are obtained by using backpropagation to jointly learn 
a representation for each word and
a function that predicts a target quantity such as the next word
in a sequence (for language modeling) or a whole sequence
of translated words (for machine translation)~\citep{Bahdanau-et-al-ICLR2015-small,Sutskever-et-al-NIPS2014}.
}
\label{fig:word-embeddings}
\end{minipage}
}
\end{figure}


%%%%%%%%%%%%%%%%%%%% 

\section{Recurrent Neural Networks}
\label{sec:rnn}

When backpropagation was first introduced, its most exciting use was
for training recurrent neural networks (RNNs). 
For tasks that involve sequential inputs, such as speech and language,
it is often better to use RNNs, illustrated in
Figure~\ref{fig:rnn}.  RNNs process an input
sequence one element at a time, maintaining in their hidden units a
``state vector'' that implicitly contains information about the
history of all the past elements of the sequence.  
When we consider the outputs of
the hidden units at different discrete time steps as if they were the
outputs of different neurons in a deep multi-layer network (right of
Figure~\ref{fig:rnn}), it becomes
clear how we can apply backpropagation to train RNNs.
% The gradient with respect to a recurrent weight,
%$w_{ij}$ that allows neuron $i$ at time $t$ to influence the activity
%of neuron $j$ at time $t+1$ can be computed by
%backpropagation-through-time (BPTT) of the error derivative for $j$ at
%time $t+1$. The full gradient with respect to $w_{ij}$ on a whole
%input sequence is just the sum of its gradients at each time-step.

RNNs are very powerful dynamical systems, but
training them proved to be problematic, because the
backpropagated gradients either grow or shrink at each time step, so
over many time-steps they typically explode or
vanish~\citep{Hochreiter91-small,Bengio-et-al-TNN1994}.
%% THE TEXT BELOW IS TOO COMPLICATED AND ALSO TOO SIMPLE: THERE ARE
%% STABLE CONDITIONS IN WHICH THE GRADIENTS DO NOT VANISH (ORTHONORMAL
%% RECURRENT MATRICES). I THINK ITS BETTER NOT TO GET INTO THIS.
%% and the conditions which allow stable dynamics also correspond to
%% vanishing gradients~\citep{Bengio-et-al-TNN1994},

Thanks to advances in their architecture~\citep{Hochreiter+Schmidhuber-1997,ElHihi+Bengio-nips8-small} 
and ways of training them~\citep{Sutskever-thesis2012,Pascanu+al-ICML2013-small}, RNNs
have been found to be very good at predicting the next character in
text~\citep{Sutskever-et-al-ICML2011} or the next word in a
sequence~\citep{Mikolov-et-al-NIPS2013} but they can also be used for
more complex tasks.  After reading an English sentence one word at a
time, for example, an English ``encoder'' network can be trained so
that the final state vector of its hidden units is a good
representation of the thought expressed by the sentence.  This thought
vector can then be used as the initial hidden state of (or as extra
input to) a jointly trained French ``decoder'' network which outputs a
probability distribution for the first word of the French
translation. If a particular first word is chosen from this
distribution and provided as input to the decoder network it will then
output a probability distribution for the second word of the
translation and so on until a full stop is
chosen~\citep{Bahdanau-et-al-ICLR2015-small,Sutskever-et-al-NIPS2014}.
Overall, this process generates sequences of French words according to
a probability distribution that depends on the English sentence.
This rather naive way of performing machine translation has quickly become
competitive with the state-of-the-art and this raises serious doubts about
whether understanding a sentence requires anything like internal symbolic
expressions that are manipulated by using inference rules.
%
%%binding their symbols to variables
%%in judiciously chosen rules of inference, as was previously thought by
%%many early AI researchers. It is 
%%The encoder-decoder translation
%%networks do not work by using internal rules of inference in which
%%variables get bound to symbols. They just use big state vectors, big weight
%%matrices and scalar non-linearities to get the job done. 
%%Deriving conclusions using only the form of the premises (which is
%%what formal logic does) is a powerful
%%technique that people can eventually master, but the translation
%%results with RNNs suggest that it may not be a good model of
%%the way we normally reason or understand language.
%%RNNs that represent thoughts as big vectors are 
It is more compatible with
the view that everyday reasoning involves many simultaneous analogies that
each contribute plausibility to a 
conclusion~\citep{Lakoff+Johnson-2008,Rogers+McClelland-book2004}.

Instead of translating the meaning of a French sentence to an English sentence,
one can learn to ``translate'' the meaning of an image into an English sentence,
as illustrated in Figure~\ref{fig:caption-generation}. The ``encoder'' here is
a deep convolutional neural network which converts the pixels into an
activity vector in its last hidden layer. The ``decoder''
is an RNN similar to the ones used for machine translation
and neural language modeling. There has been a surge of interest in such
systems recently (see citations in~\citet{Xu-et-al-arxiv2015}).
%Kiros-et-al-ICML2014,Mao+al-arxiv2014,Donahue-et-al-arxiv2014,
%              Vinyals-et-al-arxiv2014,Fang-et-al-arxiv2014,
%             Chen+Zitnick-arxiv2014,Karpathy+Li-arxiv2014,Venugopalan-et-al-arxiv2014}.

%\begin{figure}[htp*]
\begin{figure}[b]
\fbox{
\begin{minipage}{0.9\textwidth}
\centerline{\includegraphics[width=\linewidth]{caption-generation-Vinyals-et-al.png}}
%\centerline{\includegraphics[width=0.49\linewidth]{Examples_im2txt_001.png}}
%\includegraphics[width=0.49\linewidth]{Examples_im2txt_002.png}}
\centerline{\includegraphics[width=\linewidth]{good_capgen_examples.pdf}}
%\centerline{\includegraphics[width=0.49\linewidth]{Examples_im2txt_001.png}
\caption{Top: two examples of captions generated by an RNN taking
as extra input the representation extracted by a deep convolutional
net from a test image, with the RNN trained to ``translate''
high-level representations of images into captions. 
Reproduced with permission from~\citet{Vinyals-et-al-arxiv2014}.
Bottom: when the RNN is given the ability to focus its attention on a different
location in the input image (lighter = more attention) as it generates each word (underlined), we
found~\citep{Xu-et-al-arxiv2015} 
that it exploits this to achieve better ``translation'' of images into captions.
}
\label{fig:caption-generation}
\end{minipage}
}
\end{figure}


%\begin{figure}[htp*]
\begin{figure}[b]
\fbox{
\begin{minipage}{0.5\textwidth}
%\centerline{
%\includegraphics[width=0.49\linewidth]{bp-mlp1.pdf}
%\includegraphics[width=0.49\linewidth]{bp-mlp5.pdf}
%}
\centerline{
\includegraphics[width=0.9\linewidth]{fig-hidden-recurrence-rnn.pdf}
}
\caption{
%Top left: graph of feedforward computations in a multi-layer
%neural network with one input layer ($x$), two hidden layers ($h_1$ and $h_2$),
%one output layer (out), a target $y$, and an error that compares the output
%and the target. The synaptic weight matrices $W_1$, $W_2$ and $W_3$ are
%the parameters of the neural network.\newline
%Top right: back-propagation through the same multi-layer neural network
%(downward arrows compute the partial derivative of the error with respect
%to every node, i.e., the units value and the parameters).\newline
A recurrent neural network (RNN) and the unfolding in time of the
computation involved in its forward computation (right). The artificial neurons
(e.g. hidden units grouped under node $s$ with values $s_t$ at time $t$) 
get inputs from other neurons at previous time steps
(this is represented with the black square, standing for a delay of
one time step, on the left). In this way, an RNN can map an input sequence
with elements $x_t$ into an output sequence with elements $o_t$, with
each $o_t$ depending on all the previous $x_{t'}$ (for $t'\leq t$). The
same parameters (matrices $U$, $V$, $W$) are used at each time step.
Many other architectures are possible, including a variant in which
the network can generate a sequence of outputs (e.g. words), each of which
is used as inputs for the next time step.
The back-propagation algorithm (Figure~\ref{fig:backprop-box})
can be directly applied to the computational graph of the unfolded network
on the right hand side, to compute the derivative of a total error
(e.g., the log-probability of generating the right sequence of
outputs) with respect to all the states $s_t$ and all the parameters.
}
\label{fig:rnn}
\end{minipage}
}
\end{figure}


%\section{Recurrent Networks with Memory}
%% YLC: I moved this up because it connects with the RNN section above.

RNNs, once unfolded in time (Figure~\ref{fig:rnn}), can be seen as very deep
feed-forward networks in which all the layers share the same weights.
While their main purpose is to learn long-term dependencies, they
don't actually seem to be able to store any information for very
long. The dynamics of the system is such that information about the
initial state vanishes quickly. 

To correct for that, one idea is to augment the network with an
explicit memory. The first proposal of this kind is the ``Long Short
Term Memory'' networks (LSTMs) that use special hidden units whose
natural behaviour is to remember inputs for a long
time~\citep{Hochreiter+Schmidhuber-1997}.  
%A linear ``memory cell''
%interacts with other units via gated connections whose weights are
%multiplied by the state of a logistic gating unit that takes a value
%between $0$ and $1$. An input gating unit can learn when to isolate
%the cell from the effect of the outside world, and an output gating
%unit prevents the content of the memory cell to be broadcast to the
%rest of the neural network. More importantly, 
A special unit called the memory cell
acts like an accumulator or a gated leaky neuron: it has a connection
to itself at the next time-step that has a weight of 1 so it copies
its own real-valued state and accumulates the external signal, but
this self-connection is {\em multiplicatively gated} by another unit
that learns to decide when to clear the content of the memory.

%When LSTMs were first described \citep{Hochreiter+Schmidhuber-1997},
%their complexity may have been off-putting and, despite their
%impressive ability to learn very long-range temporal structure on toy
%problems, they were largely ignored until they were shown to yield
%excellent results in a cursive handwriting recognition
%competition~\citep{Graves-et-al-2009}. 
LSTM have subsequently proved to be more effective than vanilla RNNs
especially when they have several layers for each time step~\citep{Graves-et-al-ICASSP2013},
enabling an entire speech recognition system that goes
all the way from acoustics to the
sequence of characters in the transcription.  LSTMs or related forms
of gated units are also what is currently used for the encoder and
decoder networks that perform so well at machine
translation~\citep{Bahdanau-et-al-ICLR2015-small,Sutskever-et-al-NIPS2014}.

Over the last year, several authors have made different proposals to
augment RNNs with a memory module. 
%% This includes simplified
%% version of the LSTM
%% circuit~\citep{Chung-et-al-NIPSDL2014-small,Yao-et-al-SLU-workshop2014}.
%% The results suggest that not all the elements of the LSTM architecture
%% are required.
Proposals include the ``Neural Turing Machine'' in which the
network is augmented by a ``tape-like'' memory that the RNN
can choose to read from or write to~\citep{Graves-et-al-arxiv2014}, and
``Memory Networks'' in which a regular network is augmented by a
kind of associative memory~\citep{weston-memorynet-2014}. Memory
networks have yielded excellent performance on standard
question-answering benchmarks. The memory is used to remember the
story about which the network is later asked to answer questions.


%It is not yet clear which aspects of the complicated architecture of LSTMs
%are really necessary. Memory cells are a good way of allowing the
%backpropagated error derivatives to neither explode nor vanish, but they
%are not the only way.  For example, recurrent nets with rectified linear
%hidden units that use the non-linear function $y=max(0,x)$ are much simpler
%than LSTMs and if the recurrent weight matrix is initialized to be close to
%the identity matrix they work about as well as LSTMs both for toy problems
%involving very long-range temporal structure and for real tasks like
%predicting the next word in text\citep{LeHinton}.


% \section{Learning to reason and to manipulate symbols}

Beyond simple memorization, Neural Turing Machines and Memory Networks
are being used for tasks that would normally require reasoning and symbol manipulation.
Neural Turing Machines can be taught ``algorithms''. Among other
things, they can learn to output a sorted list of symbols when their
input consists of an unsorted sequence in which each symbol is
accompanied by a real value that indicates its priority in the list
~\citep{Graves-et-al-arxiv2014}.
% To achieve this it must learn a sorting program purely from examples
% of the input and output sequences
Memory Networks can be trained to keep track of the state of the world
in a setting similar to a text adventure game and after reading a story,
they can answer questions that require complex 
inference~\cite{weston-babi-arxiv-2015}.
% Following a sequence
%of actions in a virtual world described in words, in a scenario
%similar to a text adventure game, the network is shown an
%automatically generate question and taught to produce the
%corresponding answer (also generated automatically).  
In one test
example, the network is shown a 15-sentence version of the Lord of The
Rings and correctly answers questions like ``where is Frodo now?''.
~\citep{weston-memorynet-2014}.

%% Another RNN can learn to map the sequence
%% of characters in a simple program written in Python to the sequence of
%% characters that the program would
%% print~\citep{Zaremba+Sutskever-arxiv2014}, 
%% learning about {\it for loops} and small integer arithmetic.
%% Achieving this kind of behaviour seemed ludicrously optimistic
%% in the 1990's.

% Bernhard was not impressed, so maybe just cut this off
% YLC: it's true that the basic concept is more impressive than the end result.
%In a somewhat related work, a network is trained to evaluate whether a
%particular mathematical transformation of a formula is lilely to lead
%to a correct mathematical identity~\citep{zaremba-nips-2014}. The
%system was able to produce efficient identities for matrix expressions
%whose formula fill several pages.

% The renewal of interest in deep learning and recurrent neural nets is
%spurring a whole new generation of creative ideas.




\section{The future of deep learning}

Unsupervised learning~\citep{Hinton95,Salakhutdinov2009-small,VincentPLarochelleH2008-small,
koray-nips-10,gregor-icml-10,ranzato-pami,Bengio-et-al-ICML-2014,Kingma-et-al-NIPS2014} 
had a catalytic effect in reviving interest in
deep learning but has since been overshadowed by the successes of
purely supervised learning. Even though we have not focused on it
here, we expect unsupervised learning to become far more important in
the longer term.  Human and animal learning is largely unsupervised: we discover the
structure of the world by observing it, not by being told the name of
every object. 

Human vision is an active process which sequentially samples the optic
array in an intelligent, task-specific way using a small,
high-resolution fovea with a large, low-resolution surround. We expect
much of the future progress in vision to come from systems that are
trained end-to-end and combine ConvNets with RNNs that use
reinforcement learning to decide where to look. Such systems are in
their infancy but they already outperform passive vision
systems~\citep{ba+mnih} at classification tasks and produce impressive
results in learning to play many different video
games~\citep{Mnih-et-al-2015}.

Natural language understanding is another area in which deep learning
is poised to make a large impact over the next few years and we expect
systems that use RNNs to understand sentences or whole documents to learn
strategies for selectively attending to one part at a
time~\citep{Bahdanau-et-al-ICLR2015-small,Xu-et-al-arxiv2015}.

Ultimately, major progress in AI will come about through systems that
combine representation learning with complex {\em reasoning}. While
deep learning and simple reasoning have been used for speech and
handwriting recognition for a long time, new paradigms are needed to
replace rule-based manipulation of symbolic expressions by operations on
large vectors~\citep{bottou-mlj-2014}.

 
%\section{Key references}
%{\it Quite a few of these are very recent and have not yet been published
%  through a peer-review so we are giving their arxiv references}.





%\subsection{Back-propagation and Stochastic Gradient Descent}

%Rumelhart, D. E., Hinton, G. E., \& Williams, R. J. (1986)\\ Learning
%representations by back-propagating errors.\\ {\it Nature}, {\bf 323},
%533-536.
% --> \citep{Rumelhart86b}

%LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R.E., Hubbard,
%W., \& Jackel, L.D.\\ Backpropagation Applied to Handwritten Zip Code
%Recognition\\ Neural Computation, 1(4):541-551, Winter 1989
%--> \citep{LeCun89-small}

%\subsection{Distributed representations}

%Hinton, G. E., McClelland, J. L., \& Rumelhart, D. E. (1986)\\ Distributed
%representations.\\ In Rumelhart, D. E. and McClelland, J. L., editors, {\it
%  Parallel Distributed Processing: Explorations in the Microstructure of
%  Cognition. Volume 1: Foundations}, MIT Press, Cambridge, MA. pp 77-109.
% --> \citep{Hinton-et-al-PDP1986}

%Bengio, Y., Ducharme, R., \& Vincent, P. (2001)\\ A neural probabilistic
%language model.\\ In {\it NIPS 2000}, pages 932--938.
% --> \citep{BenDucVin01-short}

%% Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P. (2011)\\
%% Natural language processing (almost) from scratch.\\
%% {\it The Journal of Machine Learning Research} 12, pages 2493-2537
% --> \citep{collobert:2011b}

%Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., \& Dean
%J.\\ Distributed representations of words and phrases and their
%compositionality.\\ In {\it NIPS 2013}, pages 3111-3119.
% --> \citep{Mikolov-et-al-NIPS2013}

%Montufar, G.~F., Pascanu, R., Cho, K., and Bengio, Y. (2014).  On the
%number of linear regions of deep neural networks.\\ In {\it NIPS'2014}.
% --> \citep{Montufar-et-al-NIPS2014}.

%\subsection{The apparent limitations of backpropagation in deep networks}

%Bengio, Y., Simard, P., \& Frasconi, P. (1994)\\ Learning long-term
%dependencies with gradient descent is difficult.\\ {\it IEEE Transactions
%  on Neural Networks}, 5(2), 157-166.
% --> \citep{Bengio-et-al-TRNN1994}

%% Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio,
%% Y.\\ 
%% Identifying and attacking the saddle point problem in high-dimensional
%% non-convex optimization.\\ In {\it NIPS'2014}.

%\subsection{Deep unsupervised learning}
%Hinton, G., Osindero, S., \& Teh, Y. W. (2006)\\ A fast learning algorithm
%for deep belief nets.\\ {\it Neural Computation}, 18(7), 1527-1554.
% --> \citep{Hinton06-small}

%Erhan, D., Bengio, Y., Courville, A., Manzagol, P. A., Vincent, P., \&
%Bengio, S. (2010)\\ Why does unsupervised pre-training help deep
%learning?\\ {\it The Journal of Machine Learning Research}, {\bf 11},
%625-660.
% --> \citep{Erhan+al-2010-small}

%% Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I.,
%% Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P.,
%% Courville, A., and Bergstra, J. (2011).\\ Unsupervised and transfer learning
%% challenge: a deep learning approach.\\ In {\em JMLR W\&CP: Proc. Unsupervised
%%  and Transfer Learning\/}, volume~7.

%\subsection{Deep speech recognition}

%% Mohamed, A. R., Dahl, G. E., \& Hinton, G. (2012)\\ Acoustic modeling using deep
%% belief networks.\\ {\it IEEE Transactions on Audio, Speech, and Language
%%  Processing}, 20(1), 14-22.

%Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A. R., Jaitly, N. \&
%Kingsbury, B. (2012)\\ Deep neural networks for acoustic modeling in speech
%recognition: The shared views of four research groups.\\ {\it Signal
%  Processing Magazine}, IEEE, 29(6), 82-97.
% --> \citep{Hinton-et-al-2012}


%\subsection{Deep convolutional networks for image understanding}
%LeCun, Y., Bottou, L., Bengio, Y., \& Haffner, P. (1998)\\ Gradient-based
%learning applied to document recognition.\\ {\it Proceedings of the IEEE},
%86(11), 2278-2324.
% --> \citep{LeCun+98}

%% Jain, V., Seung, H.S., \& Turaga S.C. (2010)\\
%% Machines that learn to segment images: a crucial technology for connectomics\\
%% {\it Current opinion in neurobiology} 20 (5), 653-666.

%Krizhevsky, A., Sutskever, I., \& Hinton, G. E. (2012)\\ Imagenet
%classification with deep convolutional neural networks.\\ In Advances in
%Neural Information Processing Systems (1097-1105).
% --> \citep{Krizhevsky-2012-small}

%% Farabet, C., Couprie, C., Najman, L. \& LeCun, Y. (2013)\\
%% Learning Hierarchical Features for Scene Labeling,\\
%% IEEE Transactions on Pattern Analysis and Machine Intelligence, August 2013.

%% Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., \& LeCun,
%% Y. (2013)\\ Overfeat: Integrated recognition, localization and detection using
%% convolutional networks.\\ {\it Proceedings of ICLR 2014}, arXiv:1312.6229.

%Tompson, J., Goroshin, R., Jain, A., LeCun, Y., \& Bregler, C. (2014)
%Efficient Object Localization Using Convolutional
%Networks. arXiv:1411.4280.
% --> \citep{Tompson-et-al-arxiv2014}

%\subsection{Deep recurrent neural networks for text understanding and machine translation}

%Graves, A., Liwicki, M., Fernández, S., Bertolami, R., Bunke, H., \&
%Schmidhuber, J. (2009)\\ A novel connectionist system for unconstrained
%handwriting recognition.\\ {\it IEEE Transactions on Pattern Analysis and
%  Machine Intelligence}, 31(5), 855-868.
% --> \citep{Graves-et-al-2009}

%% Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., \& Makhoul,
% J. (2014)\\ Fast and robust neural network joint models for statistical machine
%% translation.\\ In {\it 52nd Annual Meeting of the Association for Computational
%%   Linguistics}, Baltimore, MD, USA.

%Sutskever, I., Vinyals, O., \& Le, Q. V. (2014)\\ Sequence to sequence
%learning with neural networks.\\ {\it arXiv preprint}, arXiv:1409.3215.


% --> \citep{Sutskever-et-al-NIPS2014}

%Bahdanau, D., Cho, K., \& Bengio, Y. (2014)\\ Neural machine translation by
%jointly learning to align and translate.  {\it arXiv preprint},
%arXiv:1409.0473.
% --> \citep{Bahdanau-et-al-arxiv2014}

%Vinyals, O., Toshev, A., Bengio, S., \& Erhan, D. (2014)\\ Show and Tell: A
%Neural Image Caption Generator.\\ {\it arXiv preprint}, arXiv:1411.4555.
% --> \citep{Vinyals-et-al-arxiv2014}


%% Weston, J., Chopra, S., Bordes, A. (2014)\\
%% Memory Networks. arXiv:1410.3916.

%\subsection{Deep reinforcement learning}
%Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,
%Wierstra, D., \& Riedmiller, M. (2013)\\ Playing Atari with deep
%reinforcement learning.\\ {\it arXiv preprint} arXiv:1312.5602.
% --> \citep{Deepmind-atari-arxiv2014}

%\newpage

%% \iffalse
%% \section{Boxes (TODO)}

%% A box for the forward propagation and backpropagation equations.

%% \section{Figures}

%% 1. Diagram showing:\\ a) A feedforward neural net.\\ b) A recurrent neural
%% net expanded in time.\\ c) A convolutional neural net.

%% DONE 2. Maps showing:\\ a) that word vectors learned by backpropagation are very
%% good at capturing the meanings of the words.\\ b) that the thought vectors
%% extracted by a translation model are very good at capturing the thoughts
%% expressed by sentences.

%% 3. A diagram showing how unsupervised learning can be used to pre-train a
%% deep feeedforward neural net.

%% 4. Some images with:\\ a) The opinions of a deep convolutional net about
%% the classes of the prominent objects.\\ 
%% DONE b) Several alternative captions
%% generated by using the output of the convolutional net to determine the
%% initial hidden state of a decoder recurrent net.
 
%% 5. Some images showing the localizations of parts of a person found by a
%% deep convolutional neural net.\\

%% \fi

\bibliography{strings,ml,aigaion,convnetyann,lecun}
%\bibliographystyle{natbib}
\bibliographystyle{ieeetr}

\end{document}


