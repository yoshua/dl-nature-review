\documentclass[10pts]{article}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}

\addtolength{\textwidth}{2cm}
%\addtolength{\textwidth}{1cm}
\addtolength{\textheight}{2cm}
%\addtolength{\textheight}{1cm}
\addtolength{\topmargin}{-1cm}
\addtolength{\oddsidemargin}{-1cm}
\addtolength{\evensidemargin}{-1cm}

\parskip 0.1in
\parindent 0.0in
\setcounter{secnumdepth}{1}
\title{Deep Learning}

\author{Yann LeCun, Yoshua Bengio, Geoffrey Hinton}

\begin{document}

\maketitle

\section{Preface}

Deep neural networks that learn to represent data using layers of
increasingly abstract features have dramatically improved the
state-of-the-art for speech recognition, object recognition, object
detection, predicting the activity of drug molecules, and many other
tasks. Deep learning discovers intricate structure in large datasets by
using the {\em backpropagation algorithm} to compute how the neural network
should change in order to make less errors.  Deep {\em recurrent neural
  networks} can process a sequence of inputs and produce a sequence of
outputs and can now generate good captions for images and are competitive
for language translation.

%% this comment is geoff practising using github %%%
\section{Introduction}

Deep learning is making major advances in solving problems
that have resisted the best attempts of the Artificial Intelligence
community for many years. Deep learning is very good at discovering
intricate structure in large, high-dimensional datasets and is
therefore applicable to many domains of science and business. For
example, it has beaten previous machine learning techniques at
predicting the activity of potential drug
molecules~\citep{Dahl-et-al-arxiv2014}, detecting new fundamental
particles~\citep{Melis-Higgs-boson-competition-2014}, % TODO ~\citep{YANNparticles}, 
classifying galaxies, % %TODO ~\citep{YANNgalaxies}, 
and predicting the effects of mutations
in non-coding DNA on gene expression and
disease~\citep{Keung-et-al-2014}. % Frey

So far, most applications of deep learning use feedforward neural
networks, illustrated in Figure~\ref{fig:backprop-box}, which learn
to map a fixed-size input (e.g., an image) to a fixed-size output
(e.g. a probability for each one of several candidate object categories). 
They do so by successive transformations corresponding to different
layers of artificial neurons (called {\em hidden units}),
which can be interpreted as learned derived features of the raw input. However, 
for tasks that involve sequential inputs, such as speech
and language, it is often better to use recurrent neural
networks, illustrated in Figure~\ref{fig:rnn} and discussed in Section~\ref{sec:rnn}. 
Recurrent neural networks process an input sequence one element at a time,
maintaining in their hidden units a ``state vector'' that implicitly contains information
about the history all the past elements of the sequence.
Recurrent networks are very good at predicting the next
character in text~\citep{Sutskever-et-al-ICML2011} or the next
word in a sequence~\citep{Mikolov-et-al-NIPS2013} but they can also be
used for more complex tasks.  After reading an English sentence one
word at a time, for example, an English ``encoder'' network can be
trained so that the final state vector of its hidden units is a good
representation of the thought expressed by the sentence.  This thought
vector can then be used as the initial hidden state of (or as extra
input to)  a jointly trained French ``decoder'' network which outputs a
probability distribution for the first word of the French
translation. If a particular first word is chosen 
from this distribution and provided as input to the decoder network it will
then output a probability distribution for the second word of the
translation and so on until a full stop is
chosen~\citep{Bahdanau-et-al-arxiv2014,Sutskever-et-al-NIPS2014}.
Overall, this process generates sequences of French words according to
a probability distribution that depends on the English sentence.

This rather naive way of performing machine translation has quickly become
competitive with the state-of-the-art and this raises serious doubts about
whether understanding a sentence requires anything like internal symbolic
expressions that are manipulated by binding their symbols to variables
in judiciously chosen rules of inference, as was previously thought by
many early AI researchers.  The encoder-decoder translation
networks do not work by using internal rules of inference in which
variables get bound to symbols. They just use big state vectors, big weight
matrices and scalar non-linearities to get the job done.  
Deriving conclusions using only the form of the premises (which is
what formal logic does) is a powerful
technique that people can eventually master, but the translation
results with recurrent neural networks suggest that it may not be a good model of
the way we normally reason or understand language. Recurrent neural
networks that represent thoughts as big vectors are more compatible with
the view that everyday reasoning involves many simultaneous analogies that
each contribute plausibility to a 
conclusion~\citep{Lakoff+Johnson-2008,Rogers+McClelland-book2004}.

This review is organized chronologically. We start with the introduction of
the backpropagation algorithm in the 1980's and early demonstrations
that it could learn good features for language and vision tasks.  We
then explain the actual and perceived limitations that
prevented it from living up to its early promise for so many years and how
these limitations were eventually overcome. 

\section{Pattern recognition and the back-propagation algorithm}

A neural network has input neurons that can be driven by raw input
signals such as the pixels of an image, forming an {\em input vector},
similarly to how sensory neurons are driven by outside signals like photons or sounds.
Patterns can be classified by first converting each raw input vector into a
vector of feature activations (by analogy to the response of brain neurons
to these raw signals) and then collecting evidence for each
different class by computing a differently weighted sum of the feature
activations. If good
features can be designed by hand, it is relatively easy to learn
class-specific weights that optimize
discrimination~\citep{Rosenblatt57}. Widely-used examples of such {\em linear
classifiers} are {\em logistic regression} and {\em linear support vector
machines}~\citep{bishop-book95,Boser92}.

However, even with good hand-crafted features, many interesting tasks
cannot be solved by linear classifiers. By distorting the space
as in Figure~\ref{fig:backprop-box} (top left), a linear discrimination
becomes possible.
\iffalse
A common method to get around this
problem is to transform the feature vector into a higher dimensional vector
(i.e., corresponding to a much larger set of features)
through a non-linear mapping, exploiting the fact that collections of
vectors are more likely to be easily separated when they are spread around
a high-dimensional space~\citep{Cover65}. When there are at least
as many dimensions as examples, a linear classifier can always separate the
training examples, and one clever way to obtain a very high-dimensional space is to use a
``kernel function'' that measures the similarity between two vectors.
Using the kernel function, any input vector, ${\bf x}_i$ in the training
set can be used to define a feature whose activation for a new input
vector, ${\bf x}_{\rm new}$ is just the similarity between ${\bf x}_i$ and
${\bf x}_{\rm new}$. Given a kernel function, it is possible to jointly
optimize the choice of training examplars used to define features and the
class-specific weights associated with the features~\citep{Boser92}. The training
examplars used to define features are called ``support vectors'' and Support
Vector Machines~\citep{Boser92} have been very successful, especially for
tasks with a relatively small number of training examples in which features
are individually informative. They do not, however, perform well for tasks
such as object recognition in which the input space is high-dimensional and
the input-output mapping is so complex that computing similarities using a
simple kernel function is ineffective. 
%% THE TEXT BELOW IS NICE BUT TOO MUCH DETAIL ABOUT  SVMS FOR THIS
%% REVIEW.  
%% With a general-purpose feature space
%% and kernel (such as the Gaussian kernel), generalization far from the
%% examples is poor~\citep{Bengio-localfailure-NIPS-2006-small}, which is problematic
%% in high-dimensional spaces. However, kernel machines would perform well if a
%% very good set of features were provided: learning very good
%% features is what deep learning can do.
\fi

Problems such as image and speech recognition require the input-output
function to be insensitive to irrelevant variations of the input, such as
variations in position, orientation or illumination of an object, or
variation in pitch or accent of speech, while being very sensitive to
particular minute variations, such as the difference between a white wolf
and a samoyed dog. Experimental and theoretical deep learning results 
suggest that these tasks are better handled with multiple layers of
progressively more complicated feature detectors each of which is defined
in terms of the simpler feature detectors in the previous layers.

From the earliest days of pattern recognition~\citep{selfridge,Rosenblatt57}
researchers aimed to replace hand-engineered features by multiple layers of
learned features.  This was eventually achieved by using the
backpropagation algorithm which was discovered independently by several
different groups in the 1970s and 1980s~\citep{Werbos74,Parker85,LeCun85,RHW}.  
The gradient of an objective function with respect to the network
weights tells us how a change in each of the weights would influence
the final error (top left of Figure~\ref{fig:backprop-box}).
Gradient descent changes the weights accordingly, in the direction of the
gradient, towards reducing the error.
Backpropagation is simply an application of
the chain rule of derivatives 
which yields an efficient way of computing this gradient in
a deep neural network (or any other composition of differentiable operations).
%% FOR THE NOVICE READER INCLUDING BOTH WEIGHTS AND "PARAMTERS" IS CONFUSING.
%% ITS BETTER TO SAY SOMETHING CLEARER EVEN IF IT IS LESS GENERAL OR CORRECT.
The objective function being minimized typically measures an error to be minimized: 
the discrepancy between
the actual output of the network on a given training case and the target
output for that case.  The key insight is that the error derivatives with
respect to the {\it ouputs} of a layer can be computed by working
backwards from the error derivatives with respect to its {\em inputs}
to the subsequent layer.
%% THE TRICKY STEP IS COMPUTING dE/dOUT FROM dE/dIN IN THE SUBSEQUENT
%% LAYER.  I THINK WE SHOULD FOCUS ON GETTING OVER THAT IDEA FIRST.
%%  I PLAN TO REDO THE FIGURE TO MAKE IT SIMPLER.  ITS MUCH TOO
%%  CONFUSING FOR A NOVICE AT PRESENT.  WE SHOULD HAVE A SEPARATE
%%  DISCUSSION OF THAT.
Once this backwards recursion has been used to compute
error derivatives for the outputs of all layers, it is easy to compute
error derivatives for the incoming weights of the neurons in each layer 
(Figure~\ref{fig:backprop-box}).

\begin{figure}[htp]
\fbox{
\begin{minipage}{\textwidth}
\centerline{
\fbox{\includegraphics[width=0.69\linewidth]{netvis-simple-2S.png}}
\fbox{\includegraphics[width=0.28\linewidth]{chain-rule.pdf}}}
\centerline{
\hspace*{.5mm}\fbox{\includegraphics[width=0.49\linewidth]{bpfig_forward.pdf}}
\hspace*{0mm}\fbox{\includegraphics[width=0.49\linewidth]{bpfig_backward.pdf}}
}
\caption{
{\bf Top left.} A multi-layer neural network can distort the
input space to make the classes (whose examples are supposed to
be on the red and blue lines, here) linearly separable. Note how
a regular grid in input space is also transformed. This is an
illustrative example with only two input units, two hidden units
and one output unit, but the networks used for object recognition
or natural language processing contain tens or hundreds of thousand
of units. Reproduced with permission by Chris Olah from~\url{http://colah.github.io/}.
\newline
{\bf Top right.} The chain rule of derivatives tells us how
two small effects (that of a small change of $x$ on $y$, and that of $y$ on $z$)
are composed. A small change $\Delta x$ in $x$
gets transformed first into a small change $\Delta y$ in $y$
by getting multiplied by $\frac{\partial y}{\partial x}$
(that is the definition of partial derivative). Similarly,
the change $\Delta y$ creates a change $\Delta z$ in $z$. Substituting one equation
into the other gives the chain rule of derivatives, i.e., how
$\Delta x$ gets turned into $\Delta z$ through multiplication
by the product of $\frac{\partial y}{\partial x}$ and
$\frac{\partial z}{\partial x}$. It also works when $x$, $y$
and $z$ are vectors (and the derivatives are Jacobian matrices). 
\newline
{\bf Bottom left.} 
The equations used for computing the forward pass in a neural
net with two hidden layers. At each layer, we first compute the total
input $z$ to each unit, which is a weighted sum of the outputs of the
units in the layer below. Then a non-linear function $f(.)$ is applied
to $z$ to get the output of the unit.  For simplicity, we have omitted
bias terms. The functions commonly used in neural networks include the
logistic sigmoid, $f(z) = 1/(1+\exp(-z))$, the hyberbolic tangent,
$f(z) = (\exp(z)-\exp(-z))/(\exp(z)+\exp(-z))$ and, more recently, the
rectified linear unit, $f(z) = max(0, z)$.
{\bf Bottom right.} 
The equations used for computing the backward pass. At each
hidden layer we compute the error-derivative w.r.t. the output of each
unit which is a weighted sum of the error-derivatives w.r.t. the total
inputs to the units in the layer above. We then convert the
error-derivative w.r.t. the output into the error-derivative w.r.t.
the input by multipling it by the gradient of $f(z)$.  At the output
layer, the error-derivative w.r.t. the output of a unit is computed by
differentiating the cost function. This gives $y_l-t_l$ if the cost
function for unit $l$ is $0.5(y_l-t_l)^2$ where $t_l$ is the target
value. Once the $\partial E/\partial z_k$ is known, the
error-derivative for the weight $w_{jk}$ on the connection from unit
$j$ in the layer below is just $y_j \partial E/\partial z_k$.
}
\label{fig:backprop-box}
\end{minipage}
}
\end{figure}


\section{Stochastic gradient descent}

Backpropagation provides an efficient way of computing gradients with respect to all of
the weights in a layered network on a single training case, but there are
still many ways to make use of these gradients. The most obvious method is
to average the gradients over all of the cases in the training set and then
to change each weight in proportion to its negative gradient. This
moves the weight vector in the direction of steepest descent in the
objective function. 
%However, this is needlessly slow when the 
%training set contains millions of cases and is highly redundant
%with most examples conveying no new information about the mapping from
%inputs to outputs.  
However, this is needlessly slow when the 
training set contains millions of cases because each example or small batch of
examples can be seen as a noisy (but correct in average) version of the true total gradient: a very large
number of these noisy steps can converge to a good solution
much faster than very few but expensive clean steps, because even
the clean gradient is not the optimal direction in which to go.
%
It is much faster to proceed one small ``mini-batch'' examples at a time,
i.e., compute the average gradient
on a small set of randomly chosen training cases (say 128) and to update
each weight by this stochastic estimate of its full gradient, times a
learning rate which decays as learning proceeds. Stochastic gradient
descent can be improved by using the history of the individual weight
gradients to adapt the learning rate for each weight~\citep{bottou-bousquet-2008-small}
or to provide additional increments that depend on earlier gradients 
but these techniques are still the subject of controversy and are beyond
the scope of this review (see~\citet{Montavon2012} for tricks of the trade). 
However, it is now widely accepted that some
form of stochastic gradient descent is the best bet for large
datasets.

\section{Distributed representations}

The hidden layers of a multi-layer neural network learn to represent the
network's inputs in a way that makes it easy to predict the target
outputs. This is nicely demonstrated by training a multi-layer neural
network to predict the next word in a sequence from a local context of
earlier words~\citep{BenDucVin01-short}. 
Each context word is presented to the network as a one-of-N
vector, i.e., in which one component has a value of $1$ and the rest are $0$. The
output layer has one unit per word and is forced to produce a probability
distribution over words by using the ``softmax'' function:
\begin{equation}
p_j = \frac{e^{z_j}}{\sum_i e^{z_i}}
\end{equation}
where $z_j$ is the total input received by output unit $j$. 
\iffalse
The hidden
layers use a non-linearity such as the logistic, tanh, or rectifier, respectively:
\begin{equation}
y_j = \frac{1}{1+ e^{z_j}}, \ \ \ y_j = \frac{e^{z_j}-e^{-z_j}}{e^{z_j}+e^{-z_j}}, \ \ \
 y_j = \max(0, z_j).
\end{equation}
\fi

The presence of a particular word at the $i$-th position in the input of the network 
sets the value of specialized hidden units to values that correspond to the
representation of that word. Different words create a different pattern of
activations, or word-vector, illustrated in Figure~\ref{fig:word-embeddings}.
In a language model, the other layers of the network learn to convert the input word-vectors
into an output word-vector for the predicted next-word, which can be used
to predict the probability for any word in the vocabulary to appear as the next word.
The network learns word-vectors that contain many active components
each of which can be interpreted as a separate feature of the word. This
was first demonstrated using sequences of three words, such as
(Colin, Mother, Victoria), derived from two family trees~\citep{RHW}. The learned word
vector for Colin contained ``semantic'' features representing which family
tree Colin was in, what branch of that tree he was in, and what generation
he was from. 
%% TWO NATIVE ENGLISH SPEAKERS AGREE THAT THE "FROM" SHOULD STAY AT THE
%% END IN ORDER NOT TO SOUND STILTED.
These semantic features were not explicitly present
in the input.  They were discovered by the learning procedure as a good way
of factorizing the structured relationships between the input and output
symbols into multiple ``micro-rules''.  For example, given Colin's
generation and the learned feature of Mother that the output should be one
generation higher than the input, we can already restrict the answer to
people in a specific generation. The ability of deep learning to discover
many such micro-rules allows it to generalize well to novel combinations of
individually familiar context words.

The family tree demonstration used a small, clean dataset in which the
regularities could be captured by micro-rules that were never violated and
the features were easy to interpret, but learned word-vectors also work
very well when the word sequences come from a large corpus of real text and
the individual micro-rules are unreliable~\citep{BenDucVin01-short}. When trained to
predict the next word in a news story, for example, the learned word
vectors for Tuesday and Wednesday are very similar, as are the word vectors
for Vietnam and Iraq. This is not to say that Tuesday and Wednesday are
put in the same category, but rather that they share many semantic and
grammatical attributes, so that what the neural network learns from one day of
the week can often be generalized to the other, while individual days remain
different (for example one imagines that the neural network also
learns to some features that distinguises week-end days and weekdays, while
Saturday and Sunday might have special roles in some religions, etc.).
Such representations are called {\em distributed representations} because
its elements (the features) are not mutually exclusive and there exists
a rich and vast set of possible configurations of values of these features,
corresponding to the variations seen in the observed data.
These word-vectors are thus {\em learned features} that were not determined
ahead of time by humans, but automatically discovered by the neural network.
Vector representations of words learned from text
are now very widely used in natural language 
applications~\citep{Schwenk-2007,collobert:2011b,Socher-2011,
Mikolov-et-al-NIPS2013,Bahdanau-et-al-arxiv2014,Sutskever-et-al-NIPS2014}.

The issue of representation lies at the heart of the debate between the
logic-inspired and the neurally-inspired paradigms for cognition. In the
logic-inspired paradigm, an instance of a symbol is something whose only
property is that it is either identical or non-identical to other symbol
instances. It has no internal structure that is relevant to its use. By
contrast, neural networks use large activity vectors instead of symbols and
parallel matrix operations followed by non-linearities instead of formal
rules of inference. The elements of such vectors can be understood as
the values of learned attributes (or features) associated with some object. 
With sufficient data, computational power and depth,
the combination of backpropagation for computing gradients and stochastic
gradient descent for updating the weights on connections has proved to be
very good at learning appropriate distributed representations. These
representations allow a lot of parallel computation at the feature level so
that complex tasks can be solved in a very small number of sequential
steps. In fact, it can be shown that a measure of the richness of the functions
that can be represented by a deep neural network grows exponentially
with depth~\citep{Montufar-et-al-NIPS2014}, whereas according to the
same measure, classical (shallow) non-parametric statistical methods such as
Gaussian Support Vector Machines~\citep{Bengio-localfailure-NIPS-2006-small} 
or decision trees~\citep{Bengio-decision-trees10}
would require exponential storage 
%% an exponentially large number of training examples 
%% THIS WOULD BE VERY CONFUSING FOR MOST READERS WHO WILL NOT KNOW
%% ABOUT NON-PARAMETERIC METHODS.
to represent the same kinds of functions.

\section{Language processing with deep networks}

Before the introduction of neural language
models~\citep{BenDucVin01-short}, the standard approach to statistical
modeling of language was based on counting frequencies of occurrences of
short symbol sequences (called N-grams). One disadvantage of the n-gram
approach is that the number of possible N-grams is $V^N$ where $V$ is
the vocabulary size, so taking
into account a context of more than a handful of words would require
very large training corpora. Although this can
be mitigated by only considering the more frequent of the longer sequences of 
words, it means that less frequent sequences are modeled in a 
context-blind way. N-grams
treat each word as an atomic unit, so they cannot generalize across
semantically related sequences of words. By contrast,
neural language models can generalize across semantically related
contexts because they associate each word with a vector of real
valued features
and semantically related
words end up close to each other in that vector space, as illustrated in
Figure~\ref{fig:word-embeddings}. The distributed representations of
words are obtained by using backpropagation to jointly learn 
a representation for each word and
a function that predicts a target quantity such as the next word
in a sequence (for language modeling) or a whole sequence
of translated words (for machine translation)~\citep{Devlin-et-al-ACL2014,Bahdanau-et-al-arxiv2014,
Sutskever-et-al-NIPS2014}.


\begin{figure}[ht]
\fbox{
\begin{minipage}{\textwidth}
\centerline{
\includegraphics[width=0.49\linewidth]{word-embeddings.png}
\includegraphics[width=0.49\linewidth]{phrase_zoom2.png}
}
\caption{Left: Illustration of word representations learned for modeling
language, non-linearly projected to 2-D for visualization using the
t-SNE algorithm~\citep{VanDerMaaten08}. Right: 2-D representation
of phrases learned by an English-to-French encoder-decoder recurrent
neural network~\citep{Cho-et-al-EMNLP2014}. One can observe that
semantically similar words or sequences of words are  mapped to
nearby representations.
}
\label{fig:word-embeddings}
\end{minipage}
}
\end{figure}

Instead of translating the meaning of a French sentence to an English sentence,
one can learn to ``translate'' the meaning of an image into an English sentence,
as illustrated in Figure~\ref{fig:caption-generation}. The ``encoder'' here is
a deep convolutional neural network which converts the pixels into an
activity vector in its last hidden layer. The ``decoder''
is a recurrent neural network similar to the ones used for machine translation
and neural language modeling. There has been a surge of interest in such
systems recently~\citep{Kiros-et-al-ICML2014,Mao+al-arxiv2014,Donahue-et-al-arxiv2014,
              Vinyals-et-al-arxiv2014,Fang-et-al-arxiv2014,
             Chen+Zitnick-arxiv2014,Karpathy+Li-arxiv2014,Venugopalan-et-al-arxiv2014}.

\begin{figure}[ht]
\fbox{
\begin{minipage}{\textwidth}
\centerline{\includegraphics[width=\linewidth]{caption-generation-Vinyals-et-al.png}}
\centerline{\includegraphics[width=0.49\linewidth]{Examples_im2txt_001.png}
\includegraphics[width=0.49\linewidth]{Examples_im2txt_002.png}}
\caption{Three examples of captions generated by a recurrent network taking
as extra input the representation extracted by a deep convolutional
net from a test image, with the recurrent network trained to ``translate''
high-level representations of images into captions. GT stands for ``Ground Truth'' and was labeled by humans
whereas the other text is machine-generated.
Reproduced with permission from~\citet{Vinyals-et-al-arxiv2014}.
}
\label{fig:caption-generation}
\end{minipage}
}
\end{figure}

\section{Convolutional networks}


%\begin{figure}
%\begin{center}
%  \vspace{0.2cm}
%  % \includegraphics[width=0.99\linewidth]{convnet-diagram}
%  nice convnet figure goes here
%\end{center}
%\vspace{-0.6cm}
%\caption{A typical ConvNet architecture}
%\vspace{-0.8cm}
%\label{fig:convnet}
%\end{figure}


One of the most widely used deep learning architectures is the
convolutional neural network, or ConvNet~\cite{lecun-90c,lecun-98}. ConvNets
are designed to process data that come in the form of arrays, for
example a color image composed of three 2D arrays containing pixel
intensities in the three color channels. Many data modalities are in
the form of multiple arrays: 1D for signals and language, 2D for
images or audio spectrograms, 3D for video or volumetric images.
There are four key ideas behind ConvNets that take advantage of the
properties of many such natural signals: local connections, shared
weights, pooling, and the use of many layers.

The architecture of a typical ConvNet is 
%shown in
%figure~\ref{fig-convnet}. 
structured as a series of stages. The first few stages are composed of two
types of layers: convolutional layers and pooling layers. Units in a
convolutional layer are organized in {\em feature maps}, within which
each unit is connected to local patches in the feature maps of the
previous layer through a set of weights called a {\em filter
  bank}. The result of this local weight sum is then passed through a
non-linearity.  All units in a feature map share the same filter
bank. Different feature maps in a layer use different filter banks. The
reason for this architecture is twofold. First, in array data such as
images, local groups of values are often highly correlated forming
local motifs that are easily detected. Second, the spatially local statistics of
images and other signals are invariant to location. In other word, if
a motif can appear in one part of the image, it could appear anywhere,
hence the idea of units at different locations sharing the same
weights.  Mathematically, the filtering operation performed by a
feature map is a {\em discrete convolution}, hence the name
ConvNet. 

While the role of the convolutional layer is to detect local
conjunctions of features in the previous layer, the role of the
pooling layer is to merge semantically similar features into
one. Since the relative positions of the features forming a motif can
vary somewhat, reliably detecting the motif can be done by
coarse-graining the position of each feature. A typical pooling unit
computes the max of a local patch of units in one feature map (or in a
few). Neighboring pooling units take input from patches that are
shifted by more than one row or column, thereby reducing the dimension
of the representation and creating an invariance to small shifts.  Two
or three stages of convolution, non-linearity, and pooling are
stacked, followed by more convolutional and fully-connected layers.
Back-propagating gradients through a ConvNet is as simple
as through a regular deep network, allowing all the weights in all the
filter banks to be trained. The error derivative for a shared weight
is just the sum the derivatives for all the instances of that weight.

This architecture exploits the property that many natural signals are
{\em compositional hierarchies}. In images, local combinations of
edges form motifs, motifs assemble into parts, and parts form objects.
Similar hierarchies exists in speech and text from sounds to phones,
phonemes, syllables, words, and sentences. The features that form
elements at the next level can vary in relative position and
appearance. The convolutional and pooling layers in ConvNets are
directly inspired by the classic notions of simple cells and complex
cells in visual neuroscience~\citep{Hubel62}, and the overall
architecture is reminiscent of the LGN-V1-V2-V4-IT visual cortex ventral pathway hierarchy
~\citep{Felleman+VanEssen-1991}. ConvNets have their roots in
Fukushima's Neocognitron~\citep{fukushima-82} whose architecture was
somewhat similar but did not have an end-to-end supervised learning
algorithm such as backpropagation plus stochastic gradient descent.

There have been numerous applications of convolutional networks going
back to the early 1990s. A major early success
%% THIS REWORDING AVOIDS COMPLAINTS FROM HECHT NIELSEN ETC.
was a bank check
reading system which was commercially deployed by AT\&T and NCR in
1994~\citep{lecun-98}. The system used a ConvNet trained jointly with
a probabilistic model that implemented language constraints. By the late
1990s this system was reading over 10\% of all the checks in the
US. Microsoft later deployed ConvNets in a number of OCR and handwriting
recognition
systems~\citep{simard-03,chellapilla-ist-06,chellapilla-iwfhr-06b}
including for Arabic and
Chinese~\citep{abdulkader-iwfhr-06,chellapilla-iwfhr-06a}. ConvNets
have also been used since the early 1990s for object detection in
natural images, including faces and
pedestrians~\cite{vaillant-monrocq-lecun-94,garcia-delakis-04,osadchy-07,nasse-09,sermanet-cvpr-13}.


\section{Image understanding with deep convolutional networks}

Since the early 2000s, there have been many tasks in which ConvNets
have been applied with success to the detection, segmentation, and
recognition of objects and regions in images. These were invariably
tasks in which labeled data was relatively abundant, such as traffic
sign recognition~\cite{sermanet-ijcnn-11,Ciresan-et-al-2012}, the
segmentation of biological images~\cite{ning-05} particularly for
connectomics~\cite{Turaga2010}, the detection of faces, text, and human bodies in natural
images~\cite{vaillant-monrocq-lecun-94,nowlan-platt-95,garcia-delakis-04,osadchy-07,
nasse-09,Jain-et-al-arxiv2013,sermanet-cvpr-13,Tompson-et-al-arxiv2014}.

An important application is the labeling of images at the pixel level,
whose applications include driving mobile robots and self-driving
cars~\cite{hadsell-jfr-09,farabet-icml-12}. Other applications of ConvNets that are gaining importance concern
natural language understanding~\citep{collobert:2011b}, and speech
recognition~\cite{Sainath-et-al-ICASSP2013}.

ConvNets were largely ignored by the mainline computer vision and machine
learning community until the ImageNet competition in 2012.
When deep convolutional networks were applied to a dataset of about a
million images from the web that contained a thousand different classes
they achieved spectacular results, almost halving the error rates of the
best computer vision sytems~\citep{Krizhevsky-2012-small}.  
The same kind of network is used to generate the captions shown in Figure~\ref{fig:caption-generation}.

The major factors in this
success were the large labeled dataset~\citep{imagenet_cvpr09}, the use of fast GPUs,
and the convolutional early layers.  
%% REPETITION : YB, actually it was not said earlier, only that ReLU was better than sigmoid or tanh
It had already been shown that deep
supervised nets with rectified linear units could be trained without
unsupervised pre-training~\citep{Glorot+al-AI-2011-small}. 
Rectified linear units made the
training much faster and the fully connected final layers were prevented
from overfitting the peculiarities of the training set by randomly omitting
hidden units during training~\citep{Srivastava14}. Deep convolutional nets
have quickly replaced the carefully engineered pipelines full of
hand-designed features developed by the computer vision community and they
are now deployed in many computer vision applications, including image
tagging, object detection, face recognition, facial expression recognition
and scene parsing. 

\iffalse
YANN:
CAN WE MOVE SOME OF THE LATER REFS IN THE EARLIER CONVNET SECTION TO
HERE?
\fi


%%%%%%%%%%%%%%%%%%%% 

\section{Recurrent Neural Networks}
\label{sec:rnn}

When backpropagation was first introduced, its most exciting use was for
training recurrent neural networks. These receive input vectors at each
time-step and have one or more layers of hidden units that, in addition to
having connections to higher layers or ouput units, also have connections
that allow the neurons within a layer to receive weighted input from those
same neurons at the previous time-step.  As illustrated in
Figure~\ref{fig:rnn}, when we consider the outputs of the hidden units at
different discrete time steps as if they were the outputs of different neurons
in a deep multi-layer network, it becomes clear how we can apply backpropagation
to train recurrent networks. The
the gradient with respect to a recurrent
weight, $w_{ij}$ that allows neuron $i$ at time $t$ to influence the
activity of neuron $j$ at time $t+1$ can be computed by
backpropagation-through-time (BPTT) of the error derivative for $j$ at time
$t+1$. The full gradient with respect to $w_{ij}$ on a whole input sequence is just the
sum of its gradients at each time-step.

Recurrent neural networks are very powerful dynamical systems, but training
them with BPTT proved to be problematic, because the backpropagated
gradients either grow or shrink at each time step, so over many time-steps
they typically explode or
vanish~\citep{Hochreiter91-small,Bengio-et-al-TNN1994}.
%% THE TEXT BELOW IS TOO COMPLICATED AND ALSO TOO SIMPLE: THERE ARE
%% STABLE CONDITIONS IN WHICH THE GRADIENTS DO NOT VANISH (ORTHONORMAL
%% RECURRENT MATRICES). I THINK ITS BETTER NOT TO GET INTO THIS.
%% and the conditions which allow stable dynamics also correspond to
%% vanishing gradients~\citep{Bengio-et-al-TNN1994},

\begin{figure}[H]
\fbox{
\begin{minipage}{\textwidth}
%\centerline{
%\includegraphics[width=0.49\linewidth]{bp-mlp1.pdf}
%\includegraphics[width=0.49\linewidth]{bp-mlp5.pdf}
%}
\centerline{
\includegraphics[width=0.89\linewidth]{fig-hidden-recurrence-rnn.pdf}
}
\caption{
%Top left: graph of feedforward computations in a multi-layer
%neural network with one input layer ($x$), two hidden layers ($h_1$ and $h_2$),
%one output layer (out), a target $y$, and an error that compares the output
%and the target. The synaptic weight matrices $W_1$, $W_2$ and $W_3$ are
%the parameters of the neural network.\newline
%Top right: back-propagation through the same multi-layer neural network
%(downward arrows compute the partial derivative of the error with respect
%to every node, i.e., the units value and the parameters).\newline
A recurrent neural network (left) and the unfolding in time of the
computation involved in its forward computation. The artificial neurons
(e.g. hidden units grouped under node $s$ with values $s_t$ at time $t$) 
get inputs from other neurons at previous time steps
(this is represented with the black square, standing for a delay of
one time step, on the left). In this way, a recurrent net can map an input sequence
with elements $x_t$ into an output sequence with elements $o_t$, with
each $o_t$ depending on all the previous $x_{t'}$ (for $t'\leq t$). The
same parameters (matrices $U$, $V$, $W$) are used at each time step.
Many other architectures are possible, including a variant in which
the sampled outputs (e.g. words) can be generated according to an output 
distribution (the probability of the next word, for every possible next word) 
and used as inputs for the next time step.
The back-propagation algorithm (Figure~\ref{fig:backprop-box})
can be directly applied to the computational graph of the unfolded network
on the right hand side, to compute the derivative of a total error
(e.g., the log-probability of generating the right sequence of
words) with respect to all the states $s_t$ and all the parameters.
}
\label{fig:rnn}
\end{minipage}
}
\end{figure}

\section{Overcoming the limitations of backpropagation in deep networks}

In the 1990s, the machine learning community decided that backpropagation
in deep neural networks did not work well in practice and it was largely
abandoned.  At the time, people believed that learning many layers of
non-linear feature detectors with no prior knowledge was much too difficult
for a simple gradient descent procedure and that the networks were getting
trapped in poor local minima, weight configurations for which no small
change would reduce the average error. We now know that the main reasons for the
limited success of backpropagation in deep networks were quite different:
The labeled datasets were far too small, the computers were far too slow,
and the weights were not initialised sensibly~\citep{GlorotAISTATS2010-small}.

The obvious perceived weakness of gradient descent is that it can get trapped in poor
local minima, but in high-dimensional weight-spaces this is much less
common than getting stuck close to {\em saddle points}, where the error surface
curves upwards in most directions but downwards in a few. Recent
results~\citep{Dauphin-et-al-NIPS2014-small,Choromanska-et-al-AISTATS2015} strongly suggest
that the vast majority of the critical points\footnote{where derivatives of the total
objective function with respect to the parameters vanish, and include minima,
maxima and saddle points} are saddle points, not
local minima, and that in large and deep networks almost all of the 
saddle points with predominantly upwards curvature  
give very similar performance, so it does not matter which one we find.  This goes a long way
towards explaining why stochastic gradient descent works so well for deep
neural networks.

Interest in deep feedforward networks was revived in
%% Even though I had a conference  paper in 2005, the revival was in 2006!
2006~\citep{IJCAI,Hinton06-small,Bengio-nips-2006-small,ranzato-07-small} 
by the introduction
of unsupervised learning procedures that could create layers of feature
detectors without requiring labeled data. The objective in learning each
layer of feature detectors was to be able to reconstruct or model the
activities of feature detectors (or raw inputs) in the layer below.  By
``pre-training'' several layers of progressively more complex feature
detectors using this reconstruction objective, the weights of a deep
network could be initialized to sensible values.  A final layer of output
units could then be added to the top of the network and the whole deep
system could be fine-tuned using standard
backpropagation \citep{Hinton-Science2006,Bengio-nips-2006-small,ranzato-07-small}. 
This worked remarkably well for recognizing
handwritten digits especially when the amount of labeled data was very
limited~\citep{Erhan+al-2010-small}.

The first major application of this pre-training approach was in speech
recognition and it was made possible by the advent of fast Graphics
Processing Units (GPUs) that were convenient to
program~\citep{RainaICML09-small} and allowed researchers to train
networks 10 or 20 times faster.  In 2009, the approach was used to map
short temporal windows of coefficients extracted from a soundwave to a set
of probabilities for the various fragments of speech that might be
represented by the frame in the center of the window.  It achieved
record-breaking results on a standard speech recognition benchmark that
used a small vocabulary~\citep{TIMITpaper} and was quickly developed to
give record-breaking results on a large vocabulary task~\citep{Dahl2012}.
By 2012, versions of the deep net from 2009 were being developed by many of
the major speech groups~\citep{Hinton-et-al-2012} and were already being
deployed in Android phones.  For smaller datasets, unsupervised
pre-training helps to prevent overfitting~\citep{Erhan+al-2010-small},
leading to significantly better generalization when the number of labeled
examples is small, or in a transfer setting where we have lots of
examples for some ``source'' tasks but very few for some ``target''
task. In this context, unsupervised representation learning helped to win
several transfer learning
competitions~\citep{UTLC+LISA-2011-small,Goodfellow-icml2012}.  Once deep
learning had been rehabilitated, it turned out that large labeled datasets
did not need the pre-training stage, provided the scales of the initial
random weights were carefully chosen to avoid very large or very small
initial gradients~\citep{GlorotAISTATS2010-small}. Even better
results, especially for very deep networks,
could be obtained by using rectifying non-linearities rather than
sigmoid or tanh non-linearities
~\citep{Nair-2010-small,Glorot+al-AI-2011-small,Krizhevsky-2012-small,Dahl-et-al-ICASSP2013}.

Interest in recurrent networks (which correspond to a very deep network
when unfolded in time as in Figure~\ref{fig:rnn}) was revived by
``Long Short Term Memory'' networks (LSTMs) that use special hidden units
whose natural behaviour is to remember inputs for a long
time~\citep{Hochreiter+Schmidhuber-1997}.  
A linear ``memory cell'' interacts with other units
via gated connections whose weights are multiplied by the state of a
logistic gating unit that takes a value between $0$ and $1$. An input gating
unit can learn when to isolate the cell from the effect of the outside world,
and an output gating unit prevents the content of the memory cell to be
broadcast to the rest of the neural network. More importantly, the
memory cell itself acts like an accumulator or a gated leaky neuron:  it has a connection to
itself at the next time-step that has a weight of 1 so it copies its
own real-valued state and accumulates the external signal, 
but this self-connection is {\em gated} by a ``forget gate'' so
the stored value quickly leaks away unless the forget gate has an activation
near $1$.  The gates themselves have their activities determined by
weighted inputs that they receive from the input vectors and all of the
memory cells.

When LSTMs were first described % in English
\citep{Hochreiter+Schmidhuber-1997},
their complexity may have been off-putting
and, despite their impressive ability to learn very long-range temporal
structure on toy problems, they were largely ignored until they achieved
excellent results in a competition to read cursive
hand-writing~\citep{Graves-et-al-2009}. They have subsequently proved to be very
effective for tasks that have sequential input.  When they use several
hidden layers, they are better than
deep feedforward networks for speech recognition~\citep{Graves-et-al-ICASSP2013} and
unlike feedforward nets they can implement an entire speech recognition
system that goes all the way from coefficents derived from the soundwave to
the sequence of characters in the transcription.  LSTMs or related forms of gated
units are also what is currently used for the encoder and decoder networks that
perform so well at machine
translation~\citep{Bahdanau-et-al-arxiv2014,Sutskever-et-al-NIPS2014}.

Recent work~\citep{Chung-et-al-NIPSDL2014-small,Yao-et-al-SLU-workshop2014} 
suggests that not all the elements of the LSTM
architecture are required, the crucial element being the presence and learnability 
of the forget gates which dynamically control the leaking rates  of
the memory cells.
%It is not yet clear which aspects of the complicated architecture of LSTMs
%are really necessary. Memory cells are a good way of allowing the
%backpropagated error derivatives to neither explode nor vanish, but they
%are not the only way.  For example, recurrent nets with rectified linear
%hidden units that use the non-linear function $y=max(0,x)$ are much simpler
%than LSTMs and if the recurrent weight matrix is initialized to be close to
%the identity matrix they work about as well as LSTMs both for toy problems
%involving very long-range temporal structure and for real tasks like
%predicting the next word in text\citep{LeHinton}.



\section{Deep reinforcement learning}

Learning which action will maximize the total future reward given a state
of the world is difficult when the rewards are delayed and the world has a
very large number of possible states. One impressive recent development is
a single system that can learn to play many different video
games~\citep{Deepmind-atari-arxiv2013}. The system uses a deep convolutional network to
map from the pixels on the screen at several time steps to the
expected future reward of each possible action and its only inputs are the
pixels and the current score in the game. The learning works by minimizing
the difference between the system's current estimate of future reward and a
noisy but less biased estimate that is obtained by using the same deep
network to estimate the future reward of the next state, where the next
state is chosen stochastically based on the predicted future rewards of all
the currently legal actions.

\section{Learning to process symbols}

Deep learning is now the method of choice for many perceptual tasks and is
making major inroads in natural language processing where a huge amount of
messy commonsense knowledge is required. Rapid progress is also being made
in applying deep learning, in particular BPTT, to relatively pure symbol processing tasks.
An impressive recent example is the ``Neural Turing Machine'' which is a
recurrent neural network that can choose when and what to write to or read
from an external memory~\citep{Graves-et-al-arxiv2014}. 
Among other things, this can learn to
ouput a sorted list of symbols when its input consists of an unsorted
sequence in which each symbol is accompanied by a real value that indicates
its priority in the list.  To achieve this it must learn a sorting program
purely from examples of the input and output sequences.  A different system
that also uses BPTT can learn to map the sequence of characters in a simple
program written in Python to the sequence of characters that the program
would print~\citep{Zaremba+Sutskever-arxiv2014}. 
Rather surprisingly, BPTT can learn about {\it for
  loops} and can also learn to perform addition and multiplication of
strings containing several decimal digits. Achieving this kind of behaviour
seemed ludicrously optimistic when BPTT was first proposed.
  
\section{The future of deep learning}

Human vision is an active process in which intelligent fixations are used
to ensure that only a tiny fraction of the optic array is ever processed at
high resolution. We expect much of the future progress in vision,
especially for video understanding or robotics, to come from systems that
are trained end-to-end and combine deep convolutional networks with
recurrent nets that use reinforcement learning to decide where to
look.  Such systems already outperform passive vision
systems~\citep{ba+mnih}.  Similarly, when understanding sentences or whole
documents we expect future systems to learn strategies for selectively
attending to one part at a time~\citep{Bahdanau-et-al-arxiv2014}. We also expect
that in the next few years deep recurrent neural networks will become far
better at understanding the meaning of documents, including the way in
which the thoughts expressed by the sentences are related to one another.

This review has focused on supervised learning because of its recent
achievements and we have ignored much interesting work on unsupervised
learning procedures for deep neural 
networks~\citep{Salakhutdinov2009-small,Hinton95,QuocLe-ICML2012,VincentPLarochelleH2008-small,ranzato-pami,Bengio-et-al-ICML-2014,Kingma-et-al-NIPS2014}. 
However, most data have not been hand-labeled by humans, and just like
children are not told what they should have done at each instant, we need
learning machines that can learn from mostly unlabeled examples.
In the long run,
we believe unsupervised learning will be essential for allowing
deep networks to generalize well by creating internal representations that
untangle the many different factors that interact to produce images, speech
and documents~\citep{Bengio-Courville-Vincent-TPAMI2013}.

Finally, we believe that the recent successes of deep learning using
stochastic gradient descent are only the beginning.  As pointed out
in~\citet{Bengio+Lecun-chapter2007-small}, end-to-end learning
systems that have many parameters and few built-in assumptions can make use
of more data and more compute power far more easily than systems that
require hand-engineering of domain-specific knowledge.

 
%\section{Key references}
%{\it Quite a few of these are very recent and have not yet been published
%  through a peer-review so we are giving their arxiv references}.





%\subsection{Back-propagation and Stochastic Gradient Descent}

%Rumelhart, D. E., Hinton, G. E., \& Williams, R. J. (1986)\\ Learning
%representations by back-propagating errors.\\ {\it Nature}, {\bf 323},
%533-536.
% --> \citep{Rumelhart86b}

%LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R.E., Hubbard,
%W., \& Jackel, L.D.\\ Backpropagation Applied to Handwritten Zip Code
%Recognition\\ Neural Computation, 1(4):541-551, Winter 1989
%--> \citep{LeCun89-small}

%\subsection{Distributed representations}

%Hinton, G. E., McClelland, J. L., \& Rumelhart, D. E. (1986)\\ Distributed
%representations.\\ In Rumelhart, D. E. and McClelland, J. L., editors, {\it
%  Parallel Distributed Processing: Explorations in the Microstructure of
%  Cognition. Volume 1: Foundations}, MIT Press, Cambridge, MA. pp 77-109.
% --> \citep{Hinton-et-al-PDP1986}

%Bengio, Y., Ducharme, R., \& Vincent, P. (2001)\\ A neural probabilistic
%language model.\\ In {\it NIPS 2000}, pages 932--938.
% --> \citep{BenDucVin01-short}

%% Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P. (2011)\\
%% Natural language processing (almost) from scratch.\\
%% {\it The Journal of Machine Learning Research} 12, pages 2493-2537
% --> \citep{collobert:2011b}

%Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., \& Dean
%J.\\ Distributed representations of words and phrases and their
%compositionality.\\ In {\it NIPS 2013}, pages 3111-3119.
% --> \citep{Mikolov-et-al-NIPS2013}

%Montufar, G.~F., Pascanu, R., Cho, K., and Bengio, Y. (2014).  On the
%number of linear regions of deep neural networks.\\ In {\it NIPS'2014}.
% --> \citep{Montufar-et-al-NIPS2014}.

%\subsection{The apparent limitations of backpropagation in deep networks}

%Bengio, Y., Simard, P., \& Frasconi, P. (1994)\\ Learning long-term
%dependencies with gradient descent is difficult.\\ {\it IEEE Transactions
%  on Neural Networks}, 5(2), 157-166.
% --> \citep{Bengio-et-al-TRNN1994}

%% Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio,
%% Y.\\ 
%% Identifying and attacking the saddle point problem in high-dimensional
%% non-convex optimization.\\ In {\it NIPS'2014}.

%\subsection{Deep unsupervised learning}
%Hinton, G., Osindero, S., \& Teh, Y. W. (2006)\\ A fast learning algorithm
%for deep belief nets.\\ {\it Neural Computation}, 18(7), 1527-1554.
% --> \citep{Hinton06-small}

%Erhan, D., Bengio, Y., Courville, A., Manzagol, P. A., Vincent, P., \&
%Bengio, S. (2010)\\ Why does unsupervised pre-training help deep
%learning?\\ {\it The Journal of Machine Learning Research}, {\bf 11},
%625-660.
% --> \citep{Erhan+al-2010-small}

%% Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I.,
%% Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P.,
%% Courville, A., and Bergstra, J. (2011).\\ Unsupervised and transfer learning
%% challenge: a deep learning approach.\\ In {\em JMLR W\&CP: Proc. Unsupervised
%%  and Transfer Learning\/}, volume~7.

%\subsection{Deep speech recognition}

%% Mohamed, A. R., Dahl, G. E., \& Hinton, G. (2012)\\ Acoustic modeling using deep
%% belief networks.\\ {\it IEEE Transactions on Audio, Speech, and Language
%%  Processing}, 20(1), 14-22.

%Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A. R., Jaitly, N. \&
%Kingsbury, B. (2012)\\ Deep neural networks for acoustic modeling in speech
%recognition: The shared views of four research groups.\\ {\it Signal
%  Processing Magazine}, IEEE, 29(6), 82-97.
% --> \citep{Hinton-et-al-2012}


%\subsection{Deep convolutional networks for image understanding}
%LeCun, Y., Bottou, L., Bengio, Y., \& Haffner, P. (1998)\\ Gradient-based
%learning applied to document recognition.\\ {\it Proceedings of the IEEE},
%86(11), 2278-2324.
% --> \citep{LeCun+98}

%% Jain, V., Seung, H.S., \& Turaga S.C. (2010)\\
%% Machines that learn to segment images: a crucial technology for connectomics\\
%% {\it Current opinion in neurobiology} 20 (5), 653-666.

%Krizhevsky, A., Sutskever, I., \& Hinton, G. E. (2012)\\ Imagenet
%classification with deep convolutional neural networks.\\ In Advances in
%Neural Information Processing Systems (1097-1105).
% --> \citep{Krizhevsky-2012-small}

%% Farabet, C., Couprie, C., Najman, L. \& LeCun, Y. (2013)\\
%% Learning Hierarchical Features for Scene Labeling,\\
%% IEEE Transactions on Pattern Analysis and Machine Intelligence, August 2013.

%% Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., \& LeCun,
%% Y. (2013)\\ Overfeat: Integrated recognition, localization and detection using
%% convolutional networks.\\ {\it Proceedings of ICLR 2014}, arXiv:1312.6229.

%Tompson, J., Goroshin, R., Jain, A., LeCun, Y., \& Bregler, C. (2014)
%Efficient Object Localization Using Convolutional
%Networks. arXiv:1411.4280.
% --> \citep{Tompson-et-al-arxiv2014}

%\subsection{Deep recurrent neural networks for text understanding and machine translation}

%Graves, A., Liwicki, M., Fernndez, S., Bertolami, R., Bunke, H., \&
%Schmidhuber, J. (2009)\\ A novel connectionist system for unconstrained
%handwriting recognition.\\ {\it IEEE Transactions on Pattern Analysis and
%  Machine Intelligence}, 31(5), 855-868.
% --> \citep{Graves-et-al-2009}

%% Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., \& Makhoul,
% J. (2014)\\ Fast and robust neural network joint models for statistical machine
%% translation.\\ In {\it 52nd Annual Meeting of the Association for Computational
%%   Linguistics}, Baltimore, MD, USA.

%Sutskever, I., Vinyals, O., \& Le, Q. V. (2014)\\ Sequence to sequence
%learning with neural networks.\\ {\it arXiv preprint}, arXiv:1409.3215.


% --> \citep{Sutskever-et-al-NIPS2014}

%Bahdanau, D., Cho, K., \& Bengio, Y. (2014)\\ Neural machine translation by
%jointly learning to align and translate.  {\it arXiv preprint},
%arXiv:1409.0473.
% --> \citep{Bahdanau-et-al-arxiv2014}

%Vinyals, O., Toshev, A., Bengio, S., \& Erhan, D. (2014)\\ Show and Tell: A
%Neural Image Caption Generator.\\ {\it arXiv preprint}, arXiv:1411.4555.
% --> \citep{Vinyals-et-al-arxiv2014}


%% Weston, J., Chopra, S., Bordes, A. (2014)\\
%% Memory Networks. arXiv:1410.3916.

%\subsection{Deep reinforcement learning}
%Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,
%Wierstra, D., \& Riedmiller, M. (2013)\\ Playing Atari with deep
%reinforcement learning.\\ {\it arXiv preprint} arXiv:1312.5602.
% --> \citep{Deepmind-atari-arxiv2014}

%\newpage

\iffalse
\section{Boxes (TODO)}

A box for the forward propagation and backpropagation equations.

\section{Figures}

1. Diagram showing:\\ a) A feedforward neural net.\\ b) A recurrent neural
net expanded in time.\\ c) A convolutional neural net.

DONE 2. Maps showing:\\ a) that word vectors learned by backpropagation are very
good at capturing the meanings of the words.\\ b) that the thought vectors
extracted by a translation model are very good at capturing the thoughts
expressed by sentences.

3. A diagram showing how unsupervised learning can be used to pre-train a
deep feeedforward neural net.

4. Some images with:\\ a) The opinions of a deep convolutional net about
the classes of the prominent objects.\\ 
DONE b) Several alternative captions
generated by using the output of the convolutional net to determine the
initial hidden state of a decoder recurrent net.
 
5. Some images showing the localizations of parts of a person found by a
deep convolutional neural net.\\

\fi

\bibliography{strings,ml,aigaion,convnetyann,lecun}
\bibliographystyle{natbib}

\end{document}


